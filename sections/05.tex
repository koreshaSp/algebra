\section{Лекция 14.03}
\subsection{Зависимость матрицы линейного отображения от выбранных базисов}
\begin{theorem}[О смене базисов в матрице линейного отображения]
    Пусть $L: U\mapsto V$~--- линейное отображение. 
    $e, e', f, f'$~--- базисы в $U, V$ соответственно.

    Утверждается, что тогда верна следующая формула:
    $[L]^{e'}_{f'}[u]_{e'}=[id]_{f'}^{f}[L]_f^{e}[id]_e^{e'}[u]_{e'}, \forall u\in U$.
\end{theorem}
\begin{proof}
    Для доказательства лишь необходимо вспомнить следующие определения, и тогда всё
    становится очевидно:
    \begin{itemize}
        \item $[id]_{e}^{e'}$~--- матрица преобразования базиса $e'$ к базису $e$.
        \item $[L]_f^e$~--- матрица линейного отображения $L$ в базисах $e,f$.
        \item $[id]_{f'}^f$~--- матрица преобразования базиса $f$ к базису $f'$.
    \end{itemize}
    Получается доказательство строится только исходя из определения и базовых
    свойств:
    \[
        [id]_{f'}^{f}[L]_f^{e}\underbrace{[id]_e^{e'}[u]_{e'}}_{[u]_e}=
        [id]_{f'}^{f}\underbrace{[L]_f^{e}[u]_e}_{[Lu]_f}=
        [id]_{f'}^{f}[Lu]_f=[Lu]_{f'}=[L]^{e'}_{f'}[u]_e'
    \]
\end{proof}
\begin{follow}
    $$\forall A\in M_{m\times n}(K),
    \exists C\in M_m(K), D\in M_n(K)\colon A = D^{-1}
    \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0\\
    \end{array}\right) C$$
    Более того, $D^{-1}, C$~--- обратимые матрицы.
\end{follow}
\begin{proof}
    Пусть $e,f$~--- стандартные базисы $K^n, K^m$.
    Зададим линейное отображение следующим образом $L: K^n\mapsto K^m, Lx = Ax$.

    Используя \hyperref[thm:О выборе базисов для получения элементарной матрицы линейного отображения]{
        теорему о выборе базисов для получения элементарной матрицы линейного отображения}
    можно выбрать базисы $e', f'$ пространств $K^n, K^m$ соответственно,
    что $[L]^{e'}_{f'} = \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0\\
    \end{array}\right)$.
    Теперь, используя \hyperref[thm:О смене базисов в матрице линейного отображения]
    {только что доказанную теорему},
    можно утверждать, что 
    $$\left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0\\
    \end{array}\right)=[L]^{e'}_{f'} = [id]^f_{f'}[L]^e_f[id]^{e'}_e =
    [id]^f_{f'}A[id]^{e'}_e= \left([id]^{f'}_f\right)^{-1}A \Big([id]^e_{e'}\Big)^{-1}$$
    Теперь осталось только перенести лишние множители в левую часть и получить
    искомый вид:$$[id]^{f'}_f\left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0\\
    \end{array}\right)[id]^e_{e'}=A$$
    Стоит отдельно отметить, что матрицы перехода всегда обратимы.
\end{proof}
\begin{remark}
    Такое разложение не всегда единственно. Например 
    \[
        E_n = A^{-1}E_nA, \text{верно для любых } A.
    \]
\end{remark}
\begin{definition}
    Есть две матрицы, каждая из которых разделена на 4 блока.
    $$
    A = \left(\begin{array}{c|c}
            A_{1,1} & A_{1,2}\\
            \hline
            A_{2,1} & A_{2,2}
    \end{array}\right)
    B = \left(\begin{array}{c|c}
            B_{1,1} & B_{1,2}\\
            \hline
            B_{2,1} & B_{2,2}
    \end{array}\right)$$
    Тогда определим блочное умножение матриц следующим образом:
    $$
    \left(\begin{array}{c|c}
            A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2,2}\\
            \hline
            A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{1,2} + A_{2,2}B_{2,2}\\
    \end{array}\right)
    $$
    Необходимо, чтобы размеры блоков были корректно заданы. Например $A_{i,j}, B_{i,j}\in M_n(K)$.
\end{definition}
\begin{statement}[О разложении матрицы на множители]
    $A\in M_{m\times n}$, тогда $\exists C\in M_{r\times n},
    D\in M_{m\times r}\colon A = DC$. Причём у $C$ линейно независимые строки,
    а у $D$~--- линейно независимые столбцы.
\end{statement}
\begin{proof}
    Используя только что доказанное следствие из теоремы 
    знаем, что: 
    \[
        \exists \hat{C} \in M_m(K), \hat{D} \in M_n(K)
        \colon A = \hat{D}^{-1}
        \underbrace{\left(\begin{array}{c|c}
                E_r & 0\\
                \hline
                0 & 0
        \end{array}\right)}_{\in M_{n\times m}} \hat{C} 
        = 
        \hat{D}^{-1}
        \underbrace{\left(\begin{array}{c}
                E_r\\
                \hline
                0
        \end{array}\right)}_{\in M_{n\times r}}
        \underbrace{\left(\begin{array}{c|c}
                E_r & 0 \\
        \end{array}\right)}_{\in M_{r\times m}} \hat{C} = I
    \]
    Пользуясь ассоциативностью умножения матриц, вычислим это произведение
    следующим образом:
    \[
    I = 
    \left[
        \begin{gathered}
        \hat{D}^{-1}
        \left(\begin{array}{c}
            E_r\\
            \hline
            0
        \end{array}\right) 
        =
        \left(\begin{array}{c|c}
                D & *
        \end{array}\right)
        \left(\begin{array}{c}
            E_r\\
            \hline
            0
        \end{array}\right)
        =
        D E_r + 0 = D\\
        \left(\begin{array}{c|c}
                E_r & 0
        \end{array}\right) \hat{C}
        =
        \left(\begin{array}{c|c}
                E_r & 0
        \end{array}\right)
        \left(\begin{array}{c}
            C\\
            \hline
            *
        \end{array}\right) 
        =
        C E_r + 0 = C
        \end{gathered}
    \right] = DC
    .\] 
    Теперь осталось понять почему у $D$ линейно независимые столбцы, а у $C$ линейно строки.
    Заметим, что по построению $D$ это подмножество столбцов обратимой матрицы $\hat{D}^{-1}$, 
    а $C$ это подмножество строк обратимой матрицы $\hat{C}$. А про обратимые матрицы (допустим $C$)
    мы знаем, что размерность их ядра равна нулю($\dim \ker C = 0$). Что на самом деле эквивалентно
    следующему следствию($x\in K^n$):
    $$
    C x = 0 \Rightarrow x = 0
    $$
    Таким образом мы получили линейную комбинацию столбцов, действительно, если расписать равенство
    выше следующим образом, то всё становится очевидно:
    \[
        \begin{pmatrix}
            v_1 & \dots & v_n
        \end{pmatrix} x = v_1 x_1 + \dots + v_n x_n = 0 \Rightarrow x_i = 0, \forall i
    .\] 

    Как получить линейную независимость строчек?
    Сделать это текущими методами мы не можем, но утверждается, что если все столбцы являются
    линейно независимыми, то для строчек это также верно. Докажем это в следующем разделе.
\end{proof}
\subsection{Ранги транспонированных матриц}
\begin{definition}
    Транспонированием матрицы $A\in M_{m\times n}$ назовём
    матрицу $A^T\in M_{n\times m}$ со следующим свойством:
    $(A^T)_{i,j} = A_{j,i}$.
\end{definition}
\begin{properties}
    \item $(A + B)^T = A^T + B^T$
    \item $(\lambda A)^T=\lambda A^T$
    \item $(AB)^T = B^TA^T$.
\end{properties}
\begin{theorem}[О ранге транспонированной матрицы]
    $\rk A = \rk A^T$.
\end{theorem}
\begin{proof}\leavevmode
    Из доказанного ранее знаем, что:
    $A = D^{-1}\left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0
    \end{array}\right) C$. Где $r = \rk A$. Тогда $A^T$ имеет вид:
    $A^T = C^T \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0
    \end{array}\right)(D^{-1})^T.$ Важным является факт, что $C, D$~---
    обратимые матрицы, этим мы в дальнейшем воспользуемся.
    Далее докажем несколько свойств, используя которые мы сможем понять
    $\rk A^T$.
    \begin{enumerate}
        \item Докажем $(C^T)^{-1} = (C^{-1})^T$, если $C$~--- обратимая.
            $CC^{-1} = E_n$. Тогда: 
            $(C^{-1})^TC^T=E_n^T = E_n$.
            На самом деле это значит что: если $C$~--- обратимая, то
            $C^T$~--- обратимая.
        \item Хотим понять что происходит, когда мы считаем ранг
            произведения матрицы $A$ на $B$.
            Нам будет достаточно следующего неравенства:
            $\rk AB \le min(\rk A, \rk B)$.
            Для этого докажем два факта $\rk AB \le \rk A$ и $\rk AB\le \rk B$.
            \begin{enumerate}
                \item $\rk AB \le \rk A$. 
                    Посмотрим на $AB$ как на композицию отображений $x \xrightarrow{B} Bx \xrightarrow{A} ABx$.
                    Тогда становится совсем очевидно, что $\im A \supseteq \im AB$, так как просто размерность
                    последнего отображения, не может быть меньше композиции отображений.
                \item
                    Для доказательства $\rk AB \le \rk B$ надо посмотреть, что происходит с размерностью
                    подпространств после линейного отображения. 
                    $\im AB = A(\im B)$(равенство верно просто по определению).
                    Знаем, что $\dim \im B = \rk B$.
                    Давайте поймём, что после умножения $\im B$ на $A$ ранг
                    не увеличится. Это верно, так как в если взять любой базис $B$, $e_1,\dots, e_n$,
                    то  $Ae_1, \dots, Ae_n$ будет являться образующей $A(\im B)$ просто по определению.
            \end{enumerate}
            Более того, если матрица $B$ является обратимой, то верно равенство $\rk AB = \rk A$.
            Для этого докажем неравенство в обе стороны. В одну уже доказали: $\rk AB \le \rk A$ по пункту (a).
            Доказательство в другую сторону: $\rk A = \rk ABB^{-1} \le \rk AB$(последний переход верен по пункту (b)).
            Итак, $\rk AB = A$, если $B$~--- обратимая.
    \end{enumerate}
    Тогда благодаря свойствам выше и тому, что $C^T$ и $D^T$ обратимые, следует, что умножение на них не 
    меняет ранга $A^T$, а значит:
    \[
        \rk A^T = \rk \left(C^T
        \left(\begin{array}{c|c}
                E_r & 0\\
                \hline
                0 & 0
        \end{array}\right) (D^{-1})^T\right) = 
        \rk \left(\begin{array}{c|c}
                E_r & 0\\
                \hline
                0 & 0
        \end{array}\right) = r
    \]
\end{proof}
\begin{remark}
    Заметим, что на данный момент мы избавились от долга в 
    доказательстве 
    \hyperref[stm:О разложении матрицы на множители]{утверждения о разложении матрицы на множители}
\end{remark}
\begin{example}
    Попробуем использовать наши новые знания на практике.
    Надо назначить поисковой системе набор весов так, чтобы
    мы знали важность каждого сайта в нашей сети.
    $$W_i = \sum_{j\rightarrow i} \frac{w_j}{d_j^{out}}$$
    Хотим составить систему для всех $i$ и расписать
    её через матрицу. Будем думать, что столбец соответствует
    одному сайту из которого что-то исходит. В $j$ столбце
    стоят элементы следующего вида: $0, \frac{1}{d^{out}_j}$
    (в зависимости от того соединены элементы вершиной или нет)
    это все делается для того, чтобы мы могли описать систему
    в виде: $w = P\times w$, где $w$~--- вектор, а $P$~--- матрица
    (то есть мы просто записали на языке матриц уравнение выше).

    Но есть некоторая проблема: для выполнения инварианта
    (сумма исходящих рёбер для каждой вершины равно 1) необходимо
    добавить петли с недостающем весом.

    Давайте научимся выражать и считать событие вида: 
    пользователь был на сайте $v$ и $k$ раз перешёл на случайный
    сайт с текущего и нам необходимо посчитать вероятность
    оказаться в вершине $u$. Это имеет специальное название:
    <<модель случайного блуждания на графе>>. 

    Вопрос о системе $w = P\times w$. А почему вообще у этой
    системы есть нетривиальное распределение? У нас есть
    замечательное тождество на матрицу $P$: сумма в каждом
    столбце равна 1. Запишем это в следующем виде:
    $$\big(1,1,\dots, 1\big)\times P = \big(1,1,\dots, 1\big)$$
    Ну а если транспонировать обе части, то получим:
    $$
    P^T\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 
    \begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix}
    $$
    Если перенести всё в левую часть:
    $$
    (P^T - E_n^T)\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 0
    $$
    Используя свойство транспонирования:
    $$
    (P - E_n)^T\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 0
    $$
    Отсюда $\rk(P - E_N)^T<n \Rightarrow \rk(P - E_n)<n$.
    Значит $\dim \ker(P - E_n) \ge 1$, ну а если $w\not=0$,
    а значит $Pw -  E_nw = 0\Rightarrow Pw = w$.
    Что и требовалось доказать.

    На самом деле ответ у нас очень часто ещё и единственный
    (то есть $\ker(P-E_n)=1$). Пример, когда не единственный $P = E_n$.
\end{example}
\subsection{Связь умножения матриц с элементарными преобразованиями в алгоритме Гаусса}
\begin{motivation}
    Хотим в равенстве вида:
    $A = D^{-1}\big(\dots\big)C$
    задать специальные классы матриц, такие, что если
    $C,D$ принадлежат им, то система имеет единственное решение.
\end{motivation}
\begin{definition}
    Вернетреугольной матрицей назовём матрицу вида:$$\begin{pmatrix}
        * & * & \cdots & *\\
        0 & * & \ddots & \vdots\\
        \vdots & \ddots & \ddots & *\\
        0 & \cdots& 0& *
    \end{pmatrix}
    $$
    Нижнетреугольной назовём матрицу вида: $$\begin{pmatrix}
        * & 0 & \cdots & 0\\
        * & * & \ddots & \vdots\\
        \vdots & \ddots & \ddots & 0\\
        * & \cdots& *& *
    \end{pmatrix}
    $$

    Обозначим эти классы как: $UT_n(k)$ и $LT_n(K)$ соответственно.
\end{definition}
\begin{remark}
    Заметим, что при работе метода Гаусса
    можно заменить элементарное преобразование первого типа на 
    умножение на <<почти единичную>> матрицу.
\end{remark}
\begin{definition}
    Матрица для преобразования первого типа(прибавить
    к строке $j$ строку $\lambda \cdot i$)в алгоритме
    Гаусса имеет вид: 
    \[
        E_{i,j}(\lambda) = E +
        \left(\begin{array}{cc|c|cccc}
            \text{\huge 0}&&\text{\small j}&&&&\text{\huge 0}\\
            &&&&&&\\
            &&0&&&&\\
            \hline
            \dots&0&\lambda&0&\dots&&\text{\small i}\\
            \hline
            &&0&&&\\
            \text{\huge 0}&&\vdots&&&&\text{\huge 0}\\
        \end{array}\right) 
    .\] 
    Иначе говоря это единичная матрица, в $j, i$ ячейку 
    которой добавили $\lambda$.
\end{definition}
\begin{proof}
    Заметим, что при умножении на матрицу до $i$ строчки
    (где стоит $\lambda$) ничего не меняется в результате произведения.
    Строчка $i$ даст нам то, что надо, а все строчки большие $i$ опять таки
    ничего не меняют
\end{proof}
\begin{motivation}
    Теперь хотим двигаться в сторону преобразования второго и
    третьего типа.
\end{motivation}
Утверждается, что матрица 2 типа для перестановки $i, j$ выглядит как:
\NiceMatrixOptions{code-for-first-row = \scriptstyle,code-for-first-col = \scriptstyle }
\setcounter{MaxMatrixCols}{12}
\setlength{\extrarowheight}{-3mm}
\newcommand{\blue}{\color{blue}}
\[\begin{pNiceMatrix}[last-row,first-col,nullify-dots,xdots/line-style={dashed,blue}]
& 1& & & \Vdots & & & & \Vdots \\
& & \Ddots[line-style=standard] \\
& & & 1 \\
\blue i \rightarrow & \Cdots[color=blue,line-style=dashed]& & & \blue 0 &
\Cdots & & & \blue 1 & & & \Cdots \\
& & & & & 1 \\
& & & &\Vdots & & \Ddots[line-style=standard] & & \Vdots \\
& & & & & & & 1 \\
\blue j \rightarrow & \Cdots & & & \blue 1 & \Cdots & & \Cdots & \blue 0 & & & \Cdots \\
& & & & & & & & & 1 \\
& & & & & & & & & & \Ddots[line-style=standard] \\
& & & & \Vdots & & & & \Vdots & & & 1 \\
& & & & \blue \overset{\uparrow}{i} & & & & \blue \overset{\uparrow}{j} \\
\end{pNiceMatrix}\]

А матрица 3 типа выглядит так:
\[
\begin{pmatrix}
    1 & & & &\\
      &\ddots& & & \\
      & &\lambda& & \\
      & & &\ddots& \\
      & & & &1\\
\end{pmatrix}
\]
