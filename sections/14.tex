\section{Лекция 16.05}
\begin{motivation}
    Пусть у нас есть $v_1,\dots, v_n$ базис $\R^m$.
    Хотим разложить $u = c_1 v_1 + \dots + c_n v_n$. Если $v$~--- ортонормирован,
    то такое разложение находится за $\mathcal{O}(n^2)$ операций, так как каждое
    $c_i$ просто является проекцией $\pr_{v_i}(u)$, которая ищется за $\mathcal{O}(n)$.
    А в противном случае придётся решать СЛУ за $\mathcal{O}(n^3)$.
\end{motivation}
\subsection{QR разложение и ортоногональные матрицы}

$v_1,\dots, v_n \in \R^n$. Хотим их ортогонализировать.
Составим матрицу $A = \left(v_1,\dots, v_n\right)$. Заметим, что
алгоритм Грамма-Шмидта просто применяет к этой матрице преобразования
первого типа, которые являются верхнетреугольными матрицами.
Получаем  $Q = R_1,\dots, R_n A$, где $R_i$~--- верхнетреугольные матрицы
преобразований первого типа, а $A$ исходная матрица. Так как алгоритм
получает ортонормированный базис, то получается что $Q$~--- матрица ортонормированного
набора векторов. Отметим, что все имеют единицы на главной диагонали,
а значит все $R_i$ обратимые.
Так как верхнетреугольные матрицы это кольцо и обратная к верхнетреугольной
тоже верхнетреугольная, то можно записать это же
выражение в виде $A = QR$. Такое разложение называется 
тонким $QR$ разложением.

\begin{definition}
    $Q\in M_n(\R)$ называется ортогональной, если её столбцы образуют
    ортонормированный набор векторов.
\end{definition}\begin{remark}
    Можно думать об этом же определении иначе, а именно
    если $Q^T Q = E_n$, тогда и только тогда матрица $Q$ является ортонормированной.
\end{remark}
\begin{proof}
    Заметим, что ортонормированность набора векторов равносильна
    следующему условию:
    \[
        \langle v_j, v_i \rangle = 
        \begin{cases}
            1, i = j\\
            0, i \neq j
        \end{cases}
    \] 
    Первое верно, так как $\norm{v_i} = 1, \forall i$, если $v$ ортонормированный базис.
    Второе верно, так как $\langle v_i, v_j \rangle = 0, \forall i \neq j$, если $v$ ортонормированный базис.

    Ну а уже это условия на все пары $(v_i, v_j)$ уже очевидно равносильна равенству 
    $Q^T Q = 0$, так как это то же самое, но записанное на языке матриц.
\end{proof}
\begin{properties}
        \item
            $x \mapsto Qx$ сохраняет норму.
            \begin{proof}
                $\norm{Qx}^2= x^TQ^TQx = x^Tx =  \norm{x}^2$.
            \end{proof}
        \item 
            $x \mapsto Qx$ сохраняет скалярное произведение.
            Заметим, что скалярное произведение это билинейная форма, соответствующая
            какой-то квадратичной форме, а именно норме в том же пространстве, поэтому
            это свойство следует из предыдущего.
        \item
            Пространство ортогональных матриц является группой.
\end{properties}
\begin{remark}
    Ортогональные матрицы это матрицы перехода из одного ортонормированного
    базиса в другой.
\end{remark}
\begin{proof}
    $(v_1,\dots, v_n) = (u_1,\dots, u_n)Q$, где оба базиса являются
    ортонормированными. Хотим доказать, что $Q$~-- ортогональная.
    Так как $(u_1,\dots, u_n)$ ортогональная, то обратной к ней будет
    $(u_1,\dots,u_n)^T$ просто по альтернативному определению.
    Тогда $Q = (u_1,\dots, u_n)^T \allowbreak (v_1\dots, v_n)$ является ортогональной,
    как композиция ортогональных.
\end{proof}

\subsection{Метод наименьших квадратов}
Пусть у нас есть СЛУ 
 \[
Ax = b
.\]
Если $A$ не является квадратной, а именно $A \in M_{m \times n}(\R)$.
Тогда если $m > n$, тогда $\im A \neq \R^m$. Из чего делаем вывод,
что для некоторых $b$ система и вовсе не решается.

Сейчас мы живём в следующей ситуации: мы сделали много измерений с
некоторой погрешностью, то может произойти такое, что $b$ на самом
деле лежит в $\im A$, но из-за погрешности в измерениях это не случилось.
Это приводит нас к задаче, где мы ищем следующий минимум:
$x = argmin \norm{Ax - b}^2$.

\lab{картиночка}
Нарисовав картинку понимаем, что наша задача сопряжена с нахождением проекции $b$ на $\im A$,
так как именно прообраз $\pr_{\im A} (b)$ будет являться искомым $x$.
Распишем $\norm{Ax - b}^2$ следующим образом:
\[
     \langle Ax - b, Ax - b\rangle = (Ax - b)^T(Ax - b) = x^T A^T Ax -
    x^T A^T b - \underbrace{b^TAx}_{=x^TA^Tb} + b^Tb = x^TA^TAx - 2 x^TA^Tb + b^T b
\]
На секунду представим, что мы живём в одномерье и минимизируем трёхчлен
$ax^2 - 2bx + c$ тогда минимум будет достигаться в точке $x = \frac{b}{a}$.

Тогда в многомерном варианте минимум будет достигаться в точке
(просто угадали по аналогии, а теперь докажем). Условие минимума $A^T A x = A^T b$.
Тогда точка минимума: $x_0 = (A^TA)^{-1} A^T b$.

Распишем $\norm{Ax - b}^2$ и убедимся в нашем кандидате на минимум $x_0$:
\[
    (x - x_0)^T A^T A (x - x_0) + 2x^T 
    \underbrace{A^T A x_0}_{A^T b} - x_0^T A^T A x_0 -
    2 x^T A^T b + b^T b = 
    \underbrace{(x - x_0)^TA^TA(x - x_0)}_{\norm{A(x-x_0)}^2} +
    \underbrace{x_0^TA^TAx_0 + b^T b}_{\text{не зависит от } x}
\] 
Очевидно, что в данной ситуации минимум достигается на $x = x_0$ и наша догадка верна.

Для того, чтобы $x_0$ существовало, необходимо, чтобы $A^T A$ была обратимой.
Докажем, что обратимость $A^T A$ эквивалентна линейной независимости векторов $A$.

$\ora:$\\
У $A = (v_1,\dots v_n)$ линейно независимы столбцы, тогда $v_1, \dots, v_n$ являются базисом 
некоторого подпространства $U = \langle v_1,\dots,v_n \rangle \le V$. 
Тогда $A^T A$ это матрица скалярного произведения в базисе $U$. Скалярное определение
является положительной, квадратичной формой на $V$, следовательно оно же является
положительной квадратичной формой и на $U \le V$. Следовательно, по 
\hyperref[thm:Критерий Сильвестра]{критерию Сильветра} матрица $A^T A$, являющаяся
матрицей скалярного произведения в базисе $v_1,\dots,v_n$, обратима.

$\ola:$\\
\[
    A^T A \in GL \Rightarrow \dim \ker A^T A = 0 \Rightarrow \dim \ker A = 0
\]
Второй переход выполнен, так как $\dim \ker AB \ge \dim \ker B$(см доказательство
\hyperref[thm:О ранге транспонированной матрицы]{теоремы о ранге транспонированой матрицы},
там это доказывалось в качестве подпункта в виде: $\rk AB \le \min(\rk A, \rk B)$).
Конец метода наименьших квадратов, приведём пример применения.

\begin{example}
Представим, что мы делаем следующий эксперимент: бросаем предмет
и фиксируем его положение в различные моменты времени. Получаем
следующую картинку: \lab{картинка с траекторией и точками}.

$y= ax^2 + bx + c$.
В нашем случае за неизвестную $x$ мы берём $
\begin{psmallmatrix}
    a\\ b\\ c
\end{psmallmatrix}$
Тогда оптимизируемая система выглядит как:
\[
    \begin{pmatrix}
        x_1^2 & x_1 & 1\\
        \vdots & \vdots & \vdots\\
        x_n^2 & x_n & 1\\
    \end{pmatrix}
    \begin{pmatrix}
        a \\ b \\ c
    \end{pmatrix} 
    =
    \begin{pmatrix}
        y_1\\ \vdots \\ y_n
\end{pmatrix}
\]
К сожалению в данной ситуации $A$ вышла не квадратной, но если это всё же
случилось, то Можно немного модифицировать данный метод при помощи $QR$ разложение.
\end{example}

Представим себе, что у нас матрица $A$ является квадратной. После $QR$ разложения
получаем  $A = QR$.
Тогда условие минимума из метода наименьших квадратов может быть преобразовано выгодным
нам образом:
\[
   A^T A x = A^T b \leadsto R^T \underbrace{Q Q^T}_{E} Rx = R^T Q^Tb \leadsto
   Rx = Q^Tb
\]
Благодаря тому, что $R$ обратимая(см в QR разложении), то $R^T$ 
\hyperref[thm:О ранге транспонированной матрицы]{тоже обратимая},
а значит можем на неё сократить обе части равенства.
Отметим, что $R\in UT$, следовательно эту систему можно решать за $\mathcal{O}(n^2)$.
\subsection{Операторы и билинейные формы на евклидовом пространстве}
Пусть у нас есть евклидово пространство $V$ и линейный оператор 
$L\colon V \to V$. Можно определить новую билинейную форму  двумя способами:
\begin{enumerate}
    \item $ \langle Lu, v \rangle$ 
    \item $\langle u, Lv \rangle$
\end{enumerate}
Пусть $e_1, \dots, e_n$~--- ортонормированный базис,
$A = [L]_e, x = [u]_e, y = [v]_e$.
Тогда для этих двух случаев билинейная форма выглядит как:
\begin{enumerate}
    \item $x^T A^T y$ 
    \item $x^T A y$.
\end{enumerate}
У нас выделяются два класса операторов, давайте введём формальное определение,
чтобы их обобщить.
\begin{definition}
    $L: V \to V$ называется самосопряжённым если 
    \[
        \langle Lu, v \rangle = \langle u, Lv \rangle, \forall u, v\in V
    \]
\end{definition}
\begin{definition}
    $L*$ назовём сопряжённым к $L$ если выполнено:
    $\langle u, Lv \rangle = \langle L^* u, v \rangle$
\end{definition}
\begin{remark}
    $L*$ очевидно единственный.
\end{remark}
\begin{remark}
    Если $e$ ортонормированный базис, то $[L]_e = [L*]_e^T$.
\end{remark}
\begin{follow}
    Тогда условие самосопряжённости $L$ эквивалентно $L = L^*$.
\end{follow}

\begin{statement}
    Если $L$~--- самосопряжённая, то все её собственные числа являются
    вещественными.
\end{statement}
Прежде чем перейдём к доказательству надо корректно определить 
скалярное произведение для $\mathbb{C}$ и введём новое определение.
Введём следующее скалярное произведение для комплексных чисел:
\[
    \langle x, y \rangle = \sum\limits_{i=1}^{n}{\bar{x_i}y_i} = \bar{x}^T y
\]

\begin{properties}
     \item Полуторалинейность 
     $\langle \lambda x, y \rangle = \bar{\lambda}\langle x, y \rangle$
     \item Эрмитовость
     $\overline{\langle x, y \rangle} = \langle y, x \rangle$.
\end{properties}
Получается, что в ситуации с матрицами над $\mathbb{C}$ условие
самосопряжённости будет выглядеть как $\bar{A}^T = A$.

\begin{definition}
    Далее будем называть пространство с таким скалярным произведением унитарным.
\end{definition}
\begin{definition}
    $A\in M_n(\mathbb{C})$ называется эрмитовой, если $\bar{A}^T = A$.
\end{definition}
\begin{statement}
    $A$~--- самосопряженный оператор, тогда собственные числа $A$ вещественные.
\end{statement}
\begin{proof}
    Выберем собственное число и собственный вектор $\lambda \in \mathbb{C}, v \in V$.
    \[
        \langle v, Av \rangle = \langle v, \lambda v \rangle =
        \lambda\norm{v}^2
    \]
    С другой стороны:
    \[
    \langle v, Av \rangle = \langle Av, v \rangle =
    \langle \lambda v , v \rangle = \bar{\lambda}\norm{v}^2
    \] 
    Если приравнять эти две выкладки, то получим:
    \[
        \lambda\norm{v}^2 = \bar{\lambda} \norm{v}^2
    \] 
    Откуда следует, что $\lambda \in \R$.
\end{proof}
\begin{theorem}[О существовании ортолинейного базиса из собственных векторов]
    $A$~--- самосопряжённый оператор на унитарном $V$, тогда
    $\exists$ ортогональный базис из собственных векторов $A$.
\end{theorem}
\begin{proof}
    $\lambda, v$~--- собственное число и вектор соответственно, такое точно есть так как
    мы работаем над алгебраически полным $\mathbb{C}$.
    Возьмём ортогональное дополнение $\langle v \rangle ^ \perp$ и 
    докажем, что это подпространство является инвариантным.
    Для этого проверим, что  $Au \perp v, \forall u\in \langle v \rangle ^ \perp$.
    \[
        \langle Au, v \rangle = \langle u, \underbrace{Av}_{\lambda v} \rangle 
        = \bar{\lambda} \langle u, v \rangle= 0
    \]
    Значит можем ограничить оператор $A$ на инвариантное подпространство $\langle v \rangle$
    и добавить в базис вектор $v$. Так мы строим наш базис, добавляя по одному 
    собственные вектора и сужаясь на ортогональное дополнение.
\end{proof}
\begin{follow}
    Пусть у нас есть симметричные матрицы для $\R^n$, тогда все 
    собственные числа матрицы будут вещественными. 
\end{follow}
\begin{proof}
    Латаем дырки в теореме связанные алгебраической неполнотой $\R$ при помощи
    утверждения выше.
\end{proof}

\subsection{То, что мы не успели...}
\begin{theorem}
    Если $Q \in M_n(\R)$ ортогональная, тогда собственные числа $Q$ по
    модулю равны $1$.
\end{theorem}
\begin{proof}
    $\bar{Q}^T Q= E_n$
\end{proof}

\begin{definition}
    Если для $L\colon V \to V$ выполнено:
    \[
    L L^* = L^* L
    ,\] то оно называется нормальным оператором.
\end{definition}
Для матриц такого вида тоже есть своя история.

Пусть $L\colon U \to V$, где $U, V$~--- Евклидовы пространства.
Оказывается, что всегда можно выбрать ортонормированные базисы  $f_1,\dots, f_m$,
такие, что
\[
    [L]_f = 
    \begin{pmatrix}
        \lambda_1 & & 0\\
        & \ddots & \\
        0 & & \lambda_n
    \end{pmatrix}
\] 
Такие базисы называются сингулярными.
На языке матриц это будет значить, что
\[
    A = P 
    \begin{pNiceMatrix}
        \lambda_1 & &0 \\
          & \Ddots & \\
        0& & \lambda_n\\
    \end{pNiceMatrix} Q
.\]
Такое разложение называется $SVD$ разложение.
