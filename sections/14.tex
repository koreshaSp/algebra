\section{Лекция 16.05}
Пусть у нас есть $v_1,\dots, v_n$ базис $\R^m$.
Хотим разложить $u = c_1 v_1 + \dots + c_n v_n$. Если $v$~--- ортонормирован,
то такое разложение находится за $\mathcal{O}(n)$ операций. А в противном
случае придётся решать СЛУ за $\mathcal{O}(n^3)$.
\subsection{QR разложение и ортоногональные матрицы}
$v_1,\dots, v_n \in \R^n$. Хотим их ортогонализировать.
Составим матрицу $A = \left(v_1,\dots, v_n\right)$. Заметим, что
алгоритм Грамма-Шмидта просто применяет к этой матрице преобразования
первого типа, которые являются верхнетреугольными матрицами.
Получаем  $Q = R_1,\dots, R_n A$, где $R_i$~--- верхнетреугольные матрицы
преобразований первого типа, а $A$ исходная матрица. Так как алгоритм
получает ортонормированный базис, то получается что $Q$~--- ортогональная.
Так как верхнетреугольные матрицы это кольцо и обратная к верхнетреугольной
так же верхнетреугольная, то можно записать это же
выражение в виде $A = QR$. Такое разложение называется 
тонким $QR$ разложением.

\begin{definition}
    $Q\in M_n(\R)$ называется ортогональной, если её столбцы образуют
    ортонормированный набор векторов.
\end{definition}
Можно думать об этом же определении иначе, а именно
если $Q^T Q = E_n$ то матрица $Q$ является ортонормированной.
\lab{Очевидно доказывается через определение.}

\begin{properties}
        \item
            $x \mapsto Qx$ сохраняет норму.
            \begin{proof}
                $\norm{Qx}^2= x^TQ^TQx = x^Tx =  \norm{x}^2$.
            \end{proof}
        \item 
            $x \mapsto Qx$ сохраняет скалярное произведение.
            \lab{Что-то про билинейные и квадратичные формы.}
        \item
            Пространство ортогональных матриц является группой.
\end{properties}
\begin{remark}
    Ортогональные матрицы это матрицы перехода из одного ортонормированного
    базиса в другой.
\end{remark}
\begin{proof}
    Докажем в одну сторону. \lab{доказать в другую}
    $(v_1,\dots, v_n) = (u_1,\dots, u_n)Q$, где оба базиса являются
    ортонормированными. (Хотим доказать, что $Q$~-- ортогональная).
    Так как $(u_1,\dots, u_n)$ ортогональная, то существует обратная
    ей, которая так же является ортогональной.
    Тогда $Q = (u_1,\dots, u_n)^T(v_1\dots, v_n)$ является ортогональной
    как композиция ортогональной.
\end{proof}

\subsection{Метод наименьших квадратов}
Пусть у нас есть СЛУ 
 \[
Ax = b
.\]
Если $A$ не является квадратной, а именно $A \in M_{m \times n}(\R)$.
Тогда если $m > n$, тогда $\im A \neq \R^m$.
\begin{motivation}
    Сейчас мы живём в следующей ситуации: мы сделали много измерений с
    некоторой погрешностью, то может произойти такое, что $b$ на самом
    деле лежит в $\im A$, но из-за погрешности в измерениях это не случилось.
\end{motivation}

Это приводит нас к задаче где мы ищем следующий минимум:
$x = argmin \norm{Ax - b}^2$.

\lab{картиночка}
Получается, что наша задача сопряжена с нахождением проекции $b$ на $\im A$.
Распишем
\[
    \langle Ax - b, Ax - b\rangle = (Ax - b)^T(Ax - b) = x^T A^T Ax -
    x^T A^T b - \underbrace{b^TAx}_{=x^TA^Tb} + b^Tb = x^TA^TAx - 2 x^TA^Tb + b^T b
\]
\lab{описать переходы выше}
На секунду представим, что мы живём в одномерье и минимизируем трёхчлен
$ax^2 + bx + c$ тогда минимум будет достигаться в точке $x = -\frac{b}{2a}$.

Тогда в многомерном варианте минимум будет достигаться в точке
(просто угадали, а теперь докажем). Условие минимума $A^T A x = A^T b$.
Тогда точка минимума: $x_0 = (A^TA)^{-1} A^T b$.

Распишем исходное:
\[
    (x - x_0)^T A^T A (x - x_0) + 2x^T 
    \underbrace{A^T A x_0}_{2x^T A^T b} - x_0^T A^T A x_0 -
    2 x^T A^T b + b^T b = \lab{проверить}
    \underbrace{(x - x_0)^TA^TA(x - x_0)}_{\norm{A(x-x_0)}^2} +
    \underbrace{x_0^TA^TAx_0 + b^T b}_{\text{не зависит от } x}
\] 
Для того, чтобы $x_0$ существовало необходимо, чтобы $A^T A$ была обратимой,
что эквивалентно тому, что вектора $A$ линейно независимы.
Если $A^T A$ обратима, то $\ker A = 0$, что вызывает противоречие.

Представим, что мы делаем следующий эксперимент: бросаем предмет
и фиксируем его положение в различные моменты времени. Получаем
следующую картинку: \lab{картинка с траекторией и точками}.

$y= ax^2 + bx + c$.
В нашем случае за неизвестную $x$ мы берём $
\begin{psmallmatrix}
    a\\ b\\ c
\end{psmallmatrix}$
Тогда оптимизируемая система выглядит как:
\[
    \begin{pmatrix}
        x_1^2 & x_1 & 1\\
        \vdots & \vdots & \vdots\\
        x_n^2 & x_n & 1\\
    \end{pmatrix}
    \begin{pmatrix}
        a \\ b \\ c
    \end{pmatrix} 
    =
    \begin{pmatrix}
        y_1\\ \vdots \\ y_n
\end{pmatrix}
\]
Такая система решается при помощи метода наименьших квадратов.
Можно немного модифицировать данный метод используя $QR$ разложение.
Представим себе, что у нас матрица является квадратной. После $QR$ разложения
получаем  $A = QR$.
Тогда
\[
   R^T \underbrace{Q Q^T}_{E} Rx = R^T Q^Tb,
\]
получаем, что система имеет вид:
\[
    Rx = Q^T b
\]
Так как $R$ является верхнетреугольной, благодаря этому эта система 
решается за $\mathcal{O}(n^2)$.

\subsection{Связь квадратичных форм с матрицами}
Пусть у нас есть евклидово пространство $V$ и линейный оператор 
$L\colon V \to V$. Можно определить билинейную форму  двумя способами:
\begin{enumerate}
    \item $ \langle Lu, v \rangle$ 
    \item $\langle u, Lv \rangle$
\end{enumerate}
Пусть $e_1, \dots, e_n$ ортонормированный базис ортонормированный базис,
$A = [L]_e, x = [u]_e, y = [v]_e$.
Тогда для этих двух случаев билинейная форма выглядит как:
\begin{enumerate}
    \item $x^T A^T y$ 
    \item $x^T A y$.
\end{enumerate}
У нас выделяются два класса операторов, давайте введём формальное определение,
чтобы их обобщить.
\begin{definition}
    $L: V \to V$ называется самосопряжённым если 
    \[
        \langle Lu, v \rangle = \langle u, Lv \rangle, \forall u, v\in V
    \]
\end{definition}
\begin{remark}
    Это единственный оператор, который обладает следующим свойством:
    $\langle u, Lv \rangle = \langle L^* u, v \rangle$, где $L^*$~---
    сопряжённый к $L$.
\end{remark}
\begin{follow}
    Тогда условие самосопряжённости $L$ эквивалентно $L = L^*$.
\end{follow}
\lab{Заменить $L*$ на что-то приличное}

\begin{statement}
    Если $L$~--- самосопряжённая, то все её собственные числа являются
    вещественными.
\end{statement}
Прежде чем перейдём к доказательству надо корректно определить 
скалярное произведение для $\mathbb{C}$ и введём новое определение.
Введём следующее скалярное произведение для комплексных чисел:
\[
    \sum\limits_{i=1}^{n}{\bar{x_i}y_i} = \bar{x}^T y
\]

\begin{properties}
     \item Полуторалинейность 
     $\langle \lambda x, y \rangle = \bar{\lambda}\langle x, y \rangle$
     \item
     $\overline{\langle x, y \rangle} = \langle y, x \rangle$.
\end{properties}
\lab{тут может быть подзапуталось}
Получается, что в ситуации с матрицами над $\mathbb{C}$ условие
сопряжённости будет выглядеть как $\bar{A}^T = A$.

\begin{definition}
    $A$~--- Эрмитово, если $\bar{A}^T = A$.
\end{definition}
\begin{statement}
    $A$~--- сапосопряженный оператор, тогда собственные числа $A$ вещественные.
\end{statement}
\begin{proof}
    $\lambda \in \mathbb{C}, v \in V$.
    \[
        \langle v, Av \rangle = \langle v, \lambda v \rangle =
        \lambda\norm{v}^2
    \]
    С другой стороны:
    \[
    \langle v, Av \rangle = \langle Av, v \rangle =
    \langle \lambda v , v \rangle = \bar{\lambda}\norm{v}^2
    \] 
    Если приравнять эти две выкладки, то получим:
    \[
        \lambda\norm{v}^2 = \bar{\lambda} \norm{v}^2
    \] 
    Откуда следует, что $\lambda \in \R$.
\end{proof}
\begin{definition}
    $V$~--- унитарное, если оно является аналогом Евклидового
    на комплексных числах(есть скалярное произведение).
\end{definition}
\begin{theorem}
    $A$~--- самосопряжённый оператор на унитарном $V$, тогда
    $\exists$ ортогональный базис из собственных векторов $A$.
\end{theorem}
\begin{proof}
    $\lambda, v$~--- собственные числа и вектора соответственно.
    Возьмём ортогональное дополнение $\langle v \rangle ^ \perp$ и 
    докажем, что это подпространство является инвариантным.
    Для этого проверим, что  $Au \perp v, \forall u\in \langle v \rangle ^ \perp$.
    \[
        \langle Au, v \rangle = \langle u, \underbrace{Av}_{\lambda v} \rangle = 0
    \]
    Дальше по индукции?
\end{proof}
\begin{follow}
    Пусть у нас есть симметричные матрицы для $\R^n$, тогда все 
    собственные числа матрицы будут вещественными.
\end{follow}

\subsection{То, что мы не успели...}
\begin{theorem}
    Если $Q \in M_n(\R)$ ортогональная, тогда собственные числа $Q$ по
    модулю равны $1$.
\end{theorem}
\begin{proof}
    $\bar{Q}^T Q= E_n$
\end{proof}

\begin{definition}
    Если для $L\colon V \to V$ выполнено:
    \[
    L L^* = L^* L
    ,\] то оно называется нормальным оператором.
\end{definition}
Для матриц такого вида тоже есть своя история.

Пусть $L\colon U \to V$, где $U, V$~--- Евклидовы пространства.
Оказывается, что всегда можно выбрать ортонормированные базисы  $f_1,\dots, f_m$,
такие, что
\[
    [L]_f = 
    \begin{pmatrix}
        \lambda_1 & & 0\\
        & \ddots & \\
        0 & & \lambda_n
    \end{pmatrix}
\] 
Такие базисы называются сингулярными.
На языке матриц это будет значить, что
\[
    A = P 
    \begin{pNiceMatrix}
        \lambda_1 & &0 \\
          & \Ddots & \\
        0& & \lambda_n\\
    \end{pNiceMatrix} Q
.\]
Такое разложение называется $SVD$ разложение.
