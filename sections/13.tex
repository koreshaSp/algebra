\section{Лекция 25.04}
Приведём доказательство \hyperref[thm:Формула для диагональных элементов квадратичной формы]{
    теоремы о диагональных элементах квадратичной формы
}, сформулированной на прошлой лекции.
\begin{proof}
    % TODO : перепроверить
    \begin{lemma}
        Если $A\in M_n(K), C\in UT_n(K)$ с единицами на диагонали. Тогда главные миноры
        $A$ равны главным минорам матрицы $AC$.
    \end{lemma}
    \begin{proof}
        \[
        \left(\begin{array}{c|c}
                \hat{A} & * \\
                \hline
                 *& *
        \end{array}\right)
        \begin{pmatrix}
            1 & & *\\
              &\ddots&\\
            0 & & 1
        \end{pmatrix} =
        \left(\begin{array}{c|c}
                \hat{A} & * \\
                \hline
                * & *
        \end{array}\right)
        \left(\begin{array}{cc|cc}
                \hat{C} & & &* \\
                \hline
                &&\ddots&\\
                0&&&1
        \end{array}\right)=
        \left(\begin{array}{c|c}
                \hat{A} \hat{C} & * \\
                \hline
                 * & *
        \end{array}\right)
        .\] 
        Где левый верхний блок матрицы справа имеет размер $k\times k$.
        Заметим, что на самом деле мы считаем определитель левого верхнего блока $C$ на левый верхний блок $A$.
        Тогда получаем, то  $\det \hat{A} \hat{C} = \det \hat{A}$.
    \end{proof}
    $a_{1,1} = d_1 \neq 0$.
    Будем доказывать утверждение по индукции. База индукции $k = 1$ очевидна,
    сразу перейдём к доказательству перехода $k \Rightarrow k + 1$.
    Посмотрим на главную подматрицу $A$ размера $k + 1$ $\hat{A}$
    \[
    \hat{A} = 
    \left(\begin{array}{c|c}
            \begin{array}{ccc}
                c_1 & & 0\\
                    &\ddots&\\
                0&&c_k
            \end{array} & 0 \\
            \hline
            0 & c_{k + 1}
    \end{array}\right)
    .\] 
    Получаем что, $c_1\dots c_k \cdot c_{k + 1} = d_{k + 1}\not=0$.
    Видим, что $d_k = c_1 \dots c_k \Rightarrow c_{k+1} = \frac{d_{k+1}}{d_{k}}$
\end{proof}
\begin{remark}
    Заметим, что наше условие очень похоже на 
    \hyperref[thm:LU разложение обратимой матрицы]
    {условие для существования $LU$ разложения} обратимой матрицы.
    На самом деле мы нашли $LU$ разложение.
    Оформим это подробнее в следующем утверждении.
\end{remark}
\subsection{Каноничный вид матрицы квадратичной формы и её сигнатура}
\begin{statement}[О приведении квадратичной формы к каноническому виду]
   Если все $d_i \not = 0$, тогда $A = C^T D C$, где $C\in UT_n(K)$ с единицами на диагонали, а $D$~--- диагональная.
   К тому же, существует базис в котором форма принимает следующий вид:
   $q(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k+1}^{k+l}{x_i^2}$
   для некоторых $k,l$, про которые мы поговорим позже.
\end{statement}
\begin{proof}
    Для начала стоит пояснить, почему $\exists C \in UT_n(K)$. Несложно
    заметить(по аналогии с $LU$ разложением), что так как все главные подматрицы
    обратимы, значит в \hyperref[thm:О диагонализации матрицы квадратичной формы]
    {алгоритме диагонализации матрицы квадратичной форме} используется только 
    преобразования, матрицы которых являются верхнетреугольными.

    Теперь найдём базис $y$ в котором форма будет иметь требуемый вид:
    $q(x) = \sum\limits_{i=1}^{n}{c_i x_i^2}$. 
    
   Попробуем представить, что $y_i = \sqrt{\abs{c_i}} x_i$(модуль нужен,
   так как $c_i\in \R$). У данной формулы есть проблемы:
   во-первых, мы теряем информацию о знак $c_i$, а во-вторых возникает проблема при $c_i = 0$,
   а именно в этом случае $y$ просто не будет являться системой координат.
   Очевидно, что если $c_i = 0$, тогда $y_i = x_i$ нас вполне устроит.
   Чтобы не терять информацию о знаке вид формы придётся немного усложнить.
   Приведём формулу, учитывающей знак.
   \[
       q(y) = \sum\limits_{i=1}^{k}{y_i^2} - \sum\limits_{i = k + 1}^{k + l}{y_i^2}
   \]
   В формуле выше есть небольшой подвох: а именно координаты $y$ переставлены таким 
   образом, чтобы сначала для первых $k$ координат $c_i > 0$, а затем $c_i < 0$.
\end{proof}
Теперь возникает естественный вопрос, что это за числа $k,l$ и какие на них есть ограничения?

\begin{definition}
    Сигнатурой для квадратичной формы назовём упорядоченную пару $(k,l)$. Подразумевается $K = \mathbb{R}$
\end{definition}
\begin{theorem}[Определённость сигнатуры квадратичный формы]
    Сигнатура не зависит от способа приведения к каноническому виду (Способа диагонализации).
\end{theorem}
\begin{proof}
    Покажем, что 
    \[
        k = \max_{\substack{U\le V \\ q|_U > 0}} \dim U
    .\] 
    Для начала покажем, что $k = \dim U$ для какого-то  $U\le V$, причём $q|_U > 0$
    (положительно определена на подпространстве $U$), после докажем, что не существует
    подпространства большей размерности.
    Знаем, что есть ортогональный базис $e_1,\dots,e_n$, в котором $q$ имеет следующий вид:
     \[
         q(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k + 1}^{l + k}{x^2_i}
    .\] 

    Возьмём $U = \langle e_1\dots e_k\rangle$, легко убедиться, что $q_{|U} > 0$ и $k = \dim U$.

    Теперь необходимо доказать, что не существует подпространства $U\colon
    q_{|U} > 0, \dim U \ge k + 1$
    Пусть такое $U \le V$ существует. Тогда выберем $W \le V \colon W = \langle e_{k + 1} \dots e_n\rangle$.
    Отметим, что $q_{|W} \le 0$.
    Посмотрим теперь на  $U \cap W$, оно точно ненулевое, так как $\dim W = n - k, \dim U \ge k + 1$,
    тогда просто по \hyperref[thm:Формула Грассмана]{формуле Грассмана}
    $\dim U \cap W \not= 0$.
    Значит существует  $x \not= 0\colon x\in U\cap W$. Получили противоречие, так как $q(x) > 0$ с одной стороны
    (так как $x\in U$), но $q(x) \le 0$, так как $x\in W$.
\end{proof}
\begin{follow}
    $K = \mathbb{R}, q(x) = x^T A x, d_i \not= 0$, тогда $l$ равно количеству смен знака в
    последовательности $d_0,\dots,d_n$. Более того, $k = n - l$.
\end{follow}
\\\quad
\begin{follow}
    Квадратичная форма положительно определена, если $d_i > 0, \forall i$.
\end{follow}
\\\quad
\begin{theorem}[Критерий Сильвестра]
     $K = \mathbb{R}, q(x) = x^TAx.$ 
     Тогда следующие условия эквивалентны:
     \begin{enumerate}
         \item все $d_i > 0$.
         \item $\exists C \in UT_n(\mathbb{R})$ и невырожденная, такая, что $A = C^T C$.
         \item  $\exists C$~--- невырожденная, $A = C^T C$.
         \item  $q(x)$~--- положительно определённая.
     \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
    \begin{itemize}
        \item $1 \ora 2.$\\
            $\hat{C}\in UT_n(\mathbb{R})$ c единицами на диагонали.
            По \hyperref[stm:О приведении квадратичной формы к каноническому виду]{теореме} 
            существует следующее представление $A$: $A = \hat{C}^T D \hat{C}$,
            где $c_i = \frac{d_1}{d_{i - 1}}$ находятся на диагонали $D$.
            Тогда пусть
            \[
                \sqrt{D} = 
                \begin{pmatrix}
                    \sqrt{c_1} & & 0 \\
                    & \ddots &\\
                    0 & & \sqrt{c_n}\\
                \end{pmatrix}
            \]
            Тогда $A = \left(\hat{C}^T\sqrt{D}^T\right)\left(\sqrt{D}\hat{C}\right)$, тогда 
            $C=\left(\sqrt{D}\hat{C}\right)$ и она невырожденная, как произведение 
            невырожденных.
        \item
            $2 \ora 3.$ Ослабление условия.
        \item
            $3 \ora 4.$\\
            Хотим показать, что $x^TC^T Cx \stackrel{?}{>} 0, \forall x\not=0$.
            Возьмём  $y  = Cx$, тем самым перейдя в другую систему координат.
            Заметим, что наше выражение теперь записывается как  $y^Ty \stackrel{?}{>} 0$.
            Так как $Cx \not =0$($C$~--- обратимая невырожденная матрица, $x\neq 0$),
            то $y \not= 0$, значит квадратичная форма положительно определена.
        \item
            $4 \ora 1.$\\
            $q(x) = x^T A x$. Заметим, что $k$ главная подматрица является сужением
            на подпространство $U$, задающееся первыми $k$ векторами. Поэтому так как
            $q$ положительно определённая, то $q_{|U}$ так же положительно определена.
            \\
            \begin{lemma}
                Если $q(x)  = x^T A x > 0$, тогда  $\det A > 0$.
            \end{lemma}
            \begin{proof}
                Приведём матрицу $A$ к каноническому виду. Тогда верно
                равенство $A = C^T E C$(причём $С$ невырожденная), $E$ тут получилось,
                так как $q$ положительно определена, значит $A = C^T C \Rightarrow \det A =
                \det C^T C = \underbrace{(\det C)^2}_{\not= 0} > 0$.
            \end{proof}
            Благодаря лемме из $q_{|U} > 0$ следует, что главная подматрица размера $k$
            имеет ненулевой определитель, а значит обратима.
    \end{itemize}
\end{proof}
\subsection{Евклидовы пространства}
\begin{definition}
    $V$~--- векторное пространство над $\mathbb{R}$, $\langle\cdot, \cdot\rangle\colon V\times V \mapsto R$~--- 
    скалярное произведение (билинейная положительно определённая форма). $\Rightarrow$ говорим, что на $V$ задано Евклидово пространство.
\end{definition}
\begin{motivation}
    Скалярное произведение позволяет нам строго задать понятие расстояния и даже угла в нашем пространстве.
\end{motivation}
\begin{remark}
    \[
        \begin{gathered}
        \norm{x} = \sqrt{\langle x, x \rangle}\\
        \rho(x, y) = \norm{x - y}
        \end{gathered}
    .\] 
\end{remark}
\begin{remark}
    \[
        \abs{\langle x, y \rangle} \le \norm{x} \norm{y} \Rightarrow -1 \le \frac{\langle x, y \rangle}{\norm{x}\norm{y}} \le 1
    \]
\end{remark}
\begin{definition}
    Определим $\cos \varphi = \frac{\langle x, y \rangle}{\norm{x}\norm{y}}$. $\varphi$~--- угол между $x, y$, он определён на $[0, \pi]$.
\end{definition}
\begin{motivation}
    Косинус часто применяется для проецирования.
\end{motivation}
\begin{definition}
    $v\in V$, тогда определим проекцию следующим образом: $\pr_v\colon  V\mapsto \langle v \rangle$.
\end{definition}
\begin{example}
    % TODO: расписать пример с картиночкой 
    \begin{itemize}
        \item 
    \[
        x \rightarrow\frac{\norm{x} \cdot \langle x, v \rangle }{\norm{x}\norm{v}}\cdot\frac{v}{\norm{v}}=
        \frac{ \langle x, v \rangle }{\norm{v}^2} \cdot v
     \]
        \item 
    \[
         V \to \langle v^T \rangle   
    \]        
    \[
        x \to x - Pr_v(x)
    \]
    \[
        x - \frac{\langle x, v \rangle}{\norm{v}^2} \cdot v
    \]
    \end{itemize}
\end{example}
\subsection{Ортогонализация Грама-Шмидта}
Есть некоторый набор векторов $e_1,\dots, e_k$~--- линейно независимый в Евклидовом пространстве $V$.
$f_1, \dots, f_k$~--- набор векторов такой что: 
\begin{enumerate}
    \item $f_i\perp f_j, i\not = j.$
    \item $\langle e_1,\dots, e_i \rangle = \langle f_1, \dots , f_i \rangle, \forall i$.
    \item  $\norm{f_i} = 1$
\end{enumerate}
\begin{definition}
    Ортогональный набор векторов это тот, который удовлетворяет 1.
\end{definition}
\begin{definition}
    Ортонормированный набор векторов это тот, который удовлетворяет 1,3.
\end{definition}
Процесс ортогонализации является индукционным.
База: $\langle e_1 \rangle = \langle f_1 \rangle$, тогда $f_1 = \frac{e_1}{\norm{e_1}}$. 
Переход: пусть мы уже построили набор векторов $f_1, \dots, f_{i - 1}$, хотим найти $f_i$.
Тогда утверждается, что он должен иметь вид: $f_i = c e_i + c_1 f_1 + \dots + c_{i - 1}f_{i - 1}$ 
Сделаем себе послабление и пока не будем хотеть, чтобы $\norm{f} = 1$, так как можно просто поделить потом всё 
на норму найденного вектора. 
Тогда ищем $f$ в таком виде: $f_i = e_i + c_1 f_1 + \ldots + c_{i - 1} f_{i - 1}$
Вспомним, что  $f$ должно удовлетворять: $f_i \perp f_j$, 
откуда получаем необходимое условие
\[
    0 = \langle f_i, f_j \rangle = \langle e_i, f_j \rangle + c_j \langle f_j, f_j \rangle
\]
Тогда получаем, что мы вычли из $e_i$ все возможные его проекции на $f_i$. Тогда мы получили:
\[
    c_j = - \frac{\langle e_i, f_j \rangle}{\norm{f_j}^2}
\]
Получаем, что свойство 1 мы заработали.

Поймём про свойство 2.
В индукции умеем выражать $\langle f_1, \ldots, f_{i - 1} \rangle  = \langle e_1, \ldots, e_{i - 1} \rangle$,
$e_i$ выражается через всё остальное, $f_i$ выражается через $e_i$ и всё остальное.
%TODO: возможно, надо поянсить подробнее

Для выполнения свойства 3 просто поделим каждый вектор на его норму:
 \[
     f_i = \frac{f_i}{\norm{f_i}}
.\] 
\begin{remark}
    Верно даже для бесконечных пространств, где есть правильное понятие сходимости.
\end{remark}

Пусть у нас есть $x, e_1 \dots e_k$, $U = \langle e_1, \dots, e_k \rangle$.
Хотим посчитать $\pr_U(x) = x - \ort (e_1\dots e_k, x)[k + 1]$ % TODO: тут не совсем ортогонализация, на норму тут делить не надо
(TODO: некорректная запись. На самом деле в этой записи поделили лишний раз на $\norm{x}$ при ортогонализации)

Благодаря приобретённым знаниям мы теперь умеем считать $\rho(x, U)$(расстояние от точки до подпространства)
и $\rho(x_0 + U, y_0 + W) = \rho(x_0 - y_0, U + W)$ (расстояние между двумя афинными подпространствами).
% TODO: пояснить равество расстояний
