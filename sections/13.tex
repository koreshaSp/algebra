\section{Лекция 25.04}
\begin{motivation}
    В прошлый раз обсудили билинейные и квадратичные формы.
    Докажем теорему из прошлой лекции.
\end{motivation}
\begin{proof}
    % TODO : перепроверить
    \begin{lemma}
        Если $A\in M_n(K), C\in UT_n(K)$ с единицами на диагонали. Тогда главные миноры
        $A$ равны главным минорам матрицы $AC$.
    \end{lemma}
    \begin{proof}
        \[
        \left(\begin{array}{c|c}
                \hat{A} & * \\
                \hline
                 *& *
        \end{array}\right)
        \begin{pmatrix}
            1 & & *\\
              &\ddots&\\
            0 & & 1
        \end{pmatrix} =
        \left(\begin{array}{c|c}
                \hat{A} & \\
                \hline
                 & 
        \end{array}\right)
        \left(\begin{array}{cc|cc}
                \hat{C} & & &* \\
                & & & \\
                \hline
                &&\ddots&\\
                0&&&1
        \end{array}\right)=
        \left(\begin{array}{c|c}
                \hat{A} \hat{C} & * \\
                \hline
                 * & *
        \end{array}\right)
        .\] 
        Где левый верхний блок матрицы справа имеет размер $k\times k$.
        Заметим, что на самом деле мы считаем определитель левого верхнего блока $C$ на левый верхний блок $A$.
        Тогда получаем, то  $\det \hat{A} \hat{C} = \det \hat{A}$.
    \end{proof}
    $a_{1,1} = d_1 \neq 0$.
    Будем доказывать утверждение по индукции:
    Главные миноры текущей матрицы:
    \[
    A = 
    \left(\begin{array}{c|c}
            \begin{array}{ccc}
                c_1 & & 0\\
                    &\ddots&\\
                0&&c_k
            \end{array} & 0 \\
            \hline
              0 & *
    \end{array}\right)
    .\] 
    Получаем что, $c_1\dots c_k \cdot c_{k + 1} = d_{k + 1}\not=0$.
    Видим, что $d_i = c_1\dots c_i$.  $c_i = \frac{d_i}{d_{i + 1}}$
\end{proof}
\begin{remark}
    Заметим, что наше условие очень похоже на условие для $LU$ разложение в алгоритме Гаусса. На самом деле
    мы нашли $LU$ разложение. Если это заметить, то легко догадаться до след следствия.
\end{remark}
\begin{follow}
   Если все $d_i \not = 0$, тогда $A = C^T D C$, где $C\in UT_n(K)$ с единицами на диагонали, а $D$~--- диагональная.
   $q(x) = \sum\limits_{i=1}^{n}{c_i x_i^2}$.

   \[
       \begin{pmatrix}
           c_1 & & 0\\
            & \ddots & \\
           0 & & c_n
       \end{pmatrix}
   .\] 
   Тогда получаем, что $y_i = \sqrt{\abs{c_i}} x_i$(модуль нужен, так как $c_i\in \R$). У данной формулы есть проблемы:
   во-первых, мы теряем знак $c_i$,  а во-вторых возникает проблема при $c_i = 0$.
   Очевидно, что в случае $c_i = 0$ мы получаем $y = x_i$. Приведём формулу, лишённую этих неточностей.
   \[
       g(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k + 1}^{k + l}{x_i^2}
   \]
   Теперь возникает естественный вопрос, что это за числа $k,l$ и какие на них есть ограничения?
\end{follow}
\begin{definition}
    Сигнатурой для квадратичной формы назовём упорядоченную пару $(k,l)$. Подразумевается $K = \mathbb{R}$
\end{definition}
\begin{theorem}
    Сигнатура не зависит от способа приведения к каноническому виду (Способа диагонализации).
\end{theorem}
\begin{proof}
    \[
        k = \max_{U\le V, \ g|_U > 0} \dim U
    \] Таким образом хотим показать, что $k = \dim U$ для какого-то  $U\le V$, причём $g|_U > 0$(положительно определена).
    Знаем, что есть следующее представление квадратичной формы в каком-то ортогональном базисе $e_1\dots e_n$ 
    соответствующей квадратичной форме $U$.
     \[
         q(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k + 1}^{l + k}{x^2_i}
    .\] 

    Возьмём $U = \langle e_1\dots e_k\rangle$, легко убедиться что в этом случае $k = \dim U$.

    Теперь необходимо доказать, что не существует подпространства, такое, что $\dim U \ge k + 1$. 
    Ну пусть такое $U \le V$ существует. Тогда выберем ещё и $W = \langle e_{k + 1} \dots e_n\rangle$.
    Посмотрим теперь на  $U \cap W$, оно точно ненулевое, так как $\dim W = n - k, \dim U \ge k + 1$,
    тогда просто по формуле Грассмана $\dim U \cap W \not= 0$.% TODO : ссылка на теорему 
    Значит существует  $x \not= 0\colon x\in U\cap W$. Получили противоречие, так как $q(x) > 0$ с одной стороны
    (так как $x\in U$), но $q(x) \le 0$, так как $x\in W$.
\end{proof}
\begin{follow}
    $K = \mathbb{R}, q(x) = x^T A x, d_i \not= 0$, тогда  $1 = d_0,d_1,\dots, d_n$, где $l$~--- количество смен знака в
    последовательности $d_0,\dots,d_n$.
\end{follow}
\\\quad
\begin{follow}
    Квадратичная форма положительно определена, тогда и только тогда $d_i > 0, \forall i$.
\end{follow}
\begin{proof}
    В прямую сторону это просто теорема, в обратную это просто очевидно.
\end{proof}
 \begin{theorem}
     $K = \mathbb{R}, g(x) = x^TAx.$ 
     Тогда следующие условия эквивалентны:
     \begin{enumerate}
         \item все $d_i > 0$.
         \item $\exists C \in UT_n(\mathbb{R})$ и невырожденная, такая, что $A = C^T C$.
         \item  $\exists C$~--- невырожденная, $A = C^T C$.
         \item  $g(x)$~--- положительно определённая.
     \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
    \begin{itemize}
        \item $1 \ora 2.$\\
            $\hat{C}\in UT_n(\mathbb{R})$ c единицами на диагонали.
            По теореме ($d_i \ne 0 \Rightarrow A = C^TDC$) % TODO: какой?
            существует следующее представление $A$:
            $A = \hat{C}^T D \hat{C}$, где $c_i = \frac{d_1}{d_{i - 1}}$ находятся на диагонали $D$.
            Тогда пусть
            \[
                \sqrt{D} = 
                \begin{pmatrix}
                    \sqrt{c_1} & & 0 \\
                    & \ddots &\\
                    0 & & \sqrt{c_n}\\
                \end{pmatrix}
            \]
            Тогда $A = \left(\hat{C}^T\sqrt{D}^T\right)\left(\sqrt{D}\hat{C}\right)$, тогда $A$ невырожденная как произведение 
            невырожденных.
        \item
            $2 \ora 3.$ Ослабление условия.
        \item
            $3 \ora 4.$\\
            Хотим показать, что $x^TC^T Cx \stackrel{?}{>} 0, \forall x\not=0$. Возьмём  $y  = Cx$, тем самым перейдя в другую систему 
            координат. Заметим, что наше выражение теперь записывается как  $y^Ty \stackrel{?}{>} 0$. Так как $Cx \not =0$, так как
            $C$~--- обратимая невырожденная матрица, то $y \not= 0$, значит квадратичная форма положительно определена.
        \item
            $4 \ora 1.$\\
            $q(x) = x^T A x$, что такое $k$-й главный минор? Это сужение на подпространство, задающееся первыми 
            $k$ векторами.
            Хотим показать, что у положительно определённой формы определитель тоже всегда положительный.
            \begin{lemma}
                Если $g(x)  = x^T A x > 0$, тогда  $\det A > 0$.
                Сменим базис $g(y) = \sum\limits_{i = 1}^{n}{y_i^2}$, таким образом, что $y = Cx$, где $C$~--- невырожденная матрица
                перехода. Тогда получается, что $A = C^TC, \ \det A = \det C^T C = \underbrace{(\det C)^2}_{\not= 0} > 0$.
            \end{lemma}
            %TODO: проверить что все коррексно
            Тогда  $A$ выписывается в виде $C^TEC$, тогда по лемме получаем, что определитель больше нуля.
    \end{itemize}
\end{proof}
\subsection{Евклидовы пространства}
\begin{definition}
    $V$~--- векторное пространство над $\mathbb{R}$, $\langle\cdot, \cdot\rangle\colon V\times V \mapsto R$~--- 
    скалярное произведение (билинейная положительно определённая форма). $\Rightarrow$ говорим, что на $V$ задано Евклидово пространство.
\end{definition}
\begin{motivation}
    Скалярное произведение позволяет нам строго задать понятие расстояния и даже угла в нашем пространстве.
\end{motivation}
\begin{remark}
    \[
        \begin{gathered}
        \norm{x} = \sqrt{\langle x, x \rangle}\\
        \rho(x, y) = \norm{x - y}
        \end{gathered}
    .\] 
\end{remark}
\begin{remark}
    \[
        \abs{\langle x, y \rangle} \le \norm{x} \norm{y} \Rightarrow -1 \le \frac{\langle x, y \rangle}{\norm{x}\norm{y}} \le 1
    \]
\end{remark}
\begin{definition}
    Определим $\cos \varphi = \frac{\langle x, y \rangle}{\norm{x}\norm{y}}$. $\varphi$~--- угол между $x, y$, он определён на $[0, \pi]$.
\end{definition}
\begin{motivation}
    Косинус часто применяется для проецирования.
\end{motivation}
\begin{definition}
    $v\in V$, тогда определим проекцию следующим образом: $\pr_v\colon  V\mapsto \langle v \rangle$.
\end{definition}
\begin{example}
    % TODO: расписать пример с картиночкой 
    \begin{itemize}
        \item 
    \[
        x \rightarrow\frac{\norm{x} \cdot \langle x, v \rangle }{\norm{x}\norm{v}}\cdot\frac{v}{\norm{v}}
    .\]

    \[
        x\rightarrow \frac{\langle x, v \rangle}{\norm{v}^2}\cdot v
    .\] 
        \item 
    \[
         V \to \langle v^T \rangle   
    \]        
    \[
        x \to x - Pr_v(x)
    \]
    \[
        x - \frac{\langle x, v \rangle}{\norm{v}^2} \cdot v
    \]
    \end{itemize}
\end{example}
\subsection{Ортогонализация Грама-Шмидта}
Есть некоторый набор векторов $e_1,\dots, e_k$~--- линейно независимый в Евклидовом пространстве $V$.
$f_1, \dots, f_k$~--- ? такой что: 
\begin{enumerate}
    \item $f_i\perp f_j, i\not = j.$
    \item $\langle e_1,\dots, e_i \rangle = \langle f_1, \dots , f_i \rangle, \forall i$.
    \item  $\norm{f_i} = 1$
\end{enumerate}
\begin{definition}
    Ортогональный набор векторов это тот, который удовлетворяет 1.
\end{definition}
\begin{definition}
    Ортонормированный набор векторов это тот, который удовлетворяет 1,3.
\end{definition}
Процесс ортогонализации является индукционным.
База: $\langle e_1 \rangle = \langle f_1 \rangle$, тогда $f_1 = \frac{e_1}{\norm{e_1}}$. 
Переход: пусть мы уже построили набор векторов $f_1, \dots, f_{i - 1}$, хотим найти $f_i$.
Тогда утверждается, что он должен иметь вид: $f_i = c e_i + c_1 f_1 + \dots + c_{i - 1}f_{i - 1}$ 
Сделаем себе послабление и пока не будем хотеть, чтобы $\norm{f} = 1$, так как можно просто поделить потом всё 
на норму найденного вектора. 
Тогда ищем $f$ в таком виде: $f_i = e_i + c_1 f_1 + \ldots + c_{i - 1} f_{i - 1}$
Вспомним, что  $f$ должно удовлетворять: $f_i \perp f_j$, 
откуда получаем необходимое условие
\[
    0 = \langle f_i, f_j \rangle = \langle e_i, f_j \rangle + c_j \langle f_j, f_j \rangle
\]
Тогда получаем, что мы вычли из $e_i$ все возможные его проекции на $f_i$. Тогда мы получили:
\[
    c_j = - \frac{\langle e_i, f_j \rangle}{\norm{f_j}^2}
\]
Получаем, что свойство 1 мы заработали.

Поймём про свойство 2.
В индукции умеем выражать $\langle f_1, \ldots, f_{i - 1} \rangle  = \langle e_1, \ldots, e_{i - 1} \rangle$,
$e_i$ выражается через всё остальное, $f_i$ выражается через $e_i$ и всё остальное.
%TODO: возможно, надо поянсить подробнее

Для выполнения свойства 3 просто поделим каждый вектор на его норму:
 \[
     f_i = \frac{f_i}{\norm{f_i}}
.\] 
\begin{remark}
    Верно даже для бесконечных пространств, где есть правильное понятие сходимости.
\end{remark}

Пусть у нас есть $x, e_1 \dots e_k$, $U = \langle e_1, \dots, e_k \rangle$.
Хотим посчитать $\pr_U(x) = x - \ort (e_1\dots e_k, x)[k + 1]$ % TODO: тут не совсем ортогонализация, на норму тут делить не надо
(TODO: некорректная запись. На самом деле в этой записи поделили лишний раз на $\norm{x}$ при ортогонализации)

Благодаря приобретённым знаниям мы теперь умеем считать $\rho(x, U)$(расстояние от точки до подпространства)
и $\rho(x_0 + U, y_0 + W) = \rho(x_0 - y_0, U + W)$ (расстояние между двумя афинными подпространствами).
% TODO: пояснить равество расстояний
