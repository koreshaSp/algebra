\section{Лекция 25.04}
Приведём доказательство \hyperref[thm:Формула для диагональных элементов квадратичной формы]{
    теоремы о диагональных элементах квадратичной формы
}, сформулированной на прошлой лекции.
\begin{proof}
    \begin{lemma}
        Если $A\in M_n(K), C\in UT_n(K)$ с единицами на диагонали. Тогда главные миноры
        $A$ равны главным минорам матрицы $AC$.
    \end{lemma}
    \begin{proof}
        \[
        \left(\begin{array}{c|c}
                \hat{A} & * \\
                \hline
                 *& *
        \end{array}\right)
        \begin{pmatrix}
            1 & & *\\
              &\ddots&\\
            0 & & 1
        \end{pmatrix} =
        \left(\begin{array}{c|c}
                \hat{A} & * \\
                \hline
                * & *
        \end{array}\right)
        \left(\begin{array}{cc|cc}
                \hat{C} & & &* \\
                \hline
                &&\ddots&\\
                0&&&1
        \end{array}\right)=
        \left(\begin{array}{c|c}
                \hat{A} \hat{C} & * \\
                \hline
                 * & *
        \end{array}\right)
        .\] 
        Где левый верхний блок матрицы справа имеет размер $k\times k$.
        Заметим, что на самом деле мы считаем определитель левого верхнего блока $C$ на левый верхний блок $A$.
        Тогда получаем, то  $\det \hat{A} \hat{C} = \det \hat{A} \det \hat{C} = \det \hat{A}$.
    \end{proof}
    В точности так же доказывается, что в ситуации $L\in LT$ с единицами на диагонали главные миноры
    $A, LA$ совпадают.
    Благодаря лемме теперь мы понимаем, что если в алгоритме диагонализации используется
    только преобразование с выделением полного квадрата, то миноры матрицы диагональной
    формы не будут меняться. 
    Для того, чтобы показать, что используются только преобразования
    нужного типа, покажем, что $a_{1,1} = d_1 \neq 0$.
    Будем доказывать утверждение по индукции. База индукции $k = 1$ очевидна,
    сразу перейдём к доказательству перехода $k \Rightarrow k + 1$.
    Посмотрим на главную подматрицу $A$ после $k$ совершённых преобразований первого типа
    размера $k + 1$, обозначив её за $\hat{A}$
    \[
    \hat{A} = 
    \left(\begin{array}{c|c}
            \begin{array}{ccc}
                c_1 & & 0\\
                    &\ddots&\\
                0&&c_k
            \end{array} & 0 \\
            \hline
            0 & c_{k + 1}
    \end{array}\right)
    .\] 
    Получаем что, $c_1\dots c_k \cdot c_{k + 1} = d_{k + 1}\not=0 \Rightarrow c_{k+1}\neq 0$.
    Видим, что $d_k = c_1 \dots c_k \Rightarrow c_{k+1} = \frac{d_{k+1}}{d_{k}}$
\end{proof}
\begin{remark}
    Заметим, что наше условие очень похоже на 
    \hyperref[thm:LU разложение обратимой матрицы]
    {условие для существования $LU$ разложения} обратимой матрицы.
    На самом деле мы нашли $LU$ разложение.
    Оформим это подробнее в следующем утверждении.
\end{remark}
\subsection{Каноничный вид матрицы квадратичной формы и её сигнатура}
\begin{statement}[О приведении квадратичной формы к каноническому виду]
   Если все $d_i \not = 0$, тогда $A = C^T D C$, где $C\in UT_n(K)$ с единицами на диагонали, а $D$~--- диагональная.
   К тому же, существует базис в котором форма принимает следующий вид:
   $q(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k+1}^{k+l}{x_i^2}$
   для некоторых $k,l$, про которые мы поговорим позже.
\end{statement}
\begin{proof}
    Для начала стоит пояснить, почему $\exists C \in UT_n(K)$. Несложно
    заметить(по аналогии с $LU$ разложением), что так как все главные подматрицы
    обратимы, значит в \hyperref[thm:О диагонализации матрицы квадратичной формы]
    {алгоритме диагонализации матрицы квадратичной форме} используется только 
    преобразования, матрицы которых являются верхнетреугольными.

    Теперь пусть в базисе $x$ форма будет иметь требуемый вид(такой есть
    как результат алгоритма диагонализации):
    $q(x) = \sum\limits_{i=1}^{n}{c_i x_i^2}$. 
    
   Попробуем представить, что $y_i = \sqrt{\abs{c_i}} x_i$(модуль нужен,
   так как $c_i\in \R$). У данной формулы есть проблемы:
   во-первых, мы теряем информацию о знаке $c_i$, а во-вторых возникает проблема при $c_i = 0$,
   а именно в этом случае $y$ просто не будет являться системой координат.
   Очевидно, что если $c_i = 0$, тогда $y_i = x_i$ нас вполне устроит.
   Чтобы не терять информацию о знаке вид формы придётся немного усложнить.
   Приведём формулу, учитывающую эти условности.
   \[
       q(y) = \sum\limits_{i=1}^{k}{y_i^2} - \sum\limits_{i = k + 1}^{k + l}{y_i^2}
   \]
   В формуле выше есть небольшой подвох: а именно координаты $y$ переставлены таким 
   образом, чтобы сначала для первых $k$ координат $c_i > 0$, затем для 
   следующих $l$ $c_i < 0$, а в конце $c_i = 0$.
\end{proof}
Теперь возникает естественный вопрос, что это за числа $k,l$ и какие на них есть ограничения?

\begin{definition}
    Сигнатурой для квадратичной формы назовём упорядоченную пару $(k,l)$. Подразумевается $K = \mathbb{R}$
\end{definition}
\begin{theorem}[Определённость сигнатуры квадратичный формы]
    Сигнатура не зависит от способа приведения к каноническому виду (Способа диагонализации).
\end{theorem}
\begin{proof}
    Покажем, что 
    \[
        k = \max_{\substack{U\le V \\ q|_U > 0}} \dim U
    .\] 
    Для начала покажем, что $k = \dim U$ для какого-то  $U\le V$, причём $q|_U > 0$
    (положительно определена на подпространстве $U$), после докажем, что не существует
    подпространства большей размерности.
    \hyperref[stm:О приведении квадратичной формы к каноническому виду]{По теореме выше}
    знаем, что есть ортогональный базис $e_1,\dots,e_n$, в котором $q$ имеет следующий вид:
     \[
         q(x) = \sum\limits_{i=1}^{k}{x_i^2} - \sum\limits_{i = k + 1}^{l + k}{x^2_i}
    .\] 

    Возьмём $U = \langle e_1\dots e_k\rangle$, легко убедиться, что $q_{|U} > 0$ и $k = \dim U$.

    Теперь необходимо доказать, что не существует подпространства $U\colon
    q_{|U} > 0, \dim U \ge k + 1$
    Пусть такое $U \le V$ существует. Тогда выберем $W \le V \colon W = \langle e_{k + 1} \dots e_n\rangle$.
    Отметим, что $q_{|W} \le 0$. Посмотрим теперь на  $U \cap W$, оно точно ненулевое,
    так как $\dim W = n - k, \dim U \ge k + 1$, тогда просто по 
    \hyperref[thm:Формула Грассмана]{формуле Грассмана} $\dim U \cap W \not= 0$.
    Значит существует  $x \not= 0\colon x\in U\cap W$. Получили противоречие, так как $q(x) > 0$ с одной стороны
    (так как $x\in U$), но $q(x) \le 0$, так как $x\in W$.
\end{proof}
\begin{follow}
    $K = \mathbb{R}, q(x) = x^T A x, d_i \not= 0$, тогда $l$ равно количеству смен знака в
    последовательности $d_0,\dots,d_n$. Более того, $k = n - l$.
\end{follow}
\\\quad
\begin{follow}
    Квадратичная форма положительно определена, если $d_i > 0, \forall i$.
\end{follow}
\\\quad
\begin{theorem}[Критерий Сильвестра]
     $K = \mathbb{R}, q(x) = x^TAx.$ 
     Тогда следующие условия эквивалентны:
     \begin{enumerate}
         \item все $d_i > 0$.
         \item $\exists C \in UT_n(\mathbb{R})$ и невырожденная, такая, что $A = C^T C$.
         \item  $\exists C$~--- невырожденная, $A = C^T C$.
         \item  $q(x)$~--- положительно определённая.
     \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
    \begin{itemize}
        \item $1 \ora 2.$\\
            $\hat{C}\in UT_n(\mathbb{R})$ c единицами на диагонали.
            По \hyperref[stm:О приведении квадратичной формы к каноническому виду]{теореме} 
            существует следующее представление $A$: $A = \hat{C}^T D \hat{C}$,
            где $c_i = \frac{d_1}{d_{i - 1}}$ находятся на диагонали $D$.
            Тогда пусть
            \[
                \sqrt{D} = 
                \begin{pmatrix}
                    \sqrt{c_1} & & 0 \\
                    & \ddots &\\
                    0 & & \sqrt{c_n}\\
                \end{pmatrix}
            \]
            Тогда $A = \left(\hat{C}^T\sqrt{D}^T\right)\left(\sqrt{D}\hat{C}\right)$, тогда 
            $C=\left(\sqrt{D}\hat{C}\right)$ и она невырожденная, как произведение 
            невырожденных.
        \item
            $2 \ora 3.$ Ослабление условия.
        \item
            $3 \ora 4.$\\
            Хотим показать, что $x^TC^T Cx \stackrel{?}{>} 0, \forall x\not=0$.
            Возьмём  $y  = Cx$, тем самым перейдя в другую систему координат.
            Заметим, что наше выражение теперь записывается как  $y^Ty \stackrel{?}{>} 0$.
            Отличный факт: $y^T y = \sum y_i^2 \Rightarrow y^T y \ge 0$.
            Так как $Cx \not =0$($C$~--- обратимая невырожденная матрица, $x\neq 0$),
            то $y \not= 0 \Rightarrow y^T y\neq 0$, значит квадратичная форма положительно определена.
        \item
            $4 \ora 1.$\\
            $q(x) = x^T A x$. Заметим, что $k$ главная подматрица является сужением
            на подпространство $U$, задающееся первыми $k$ векторами. Поэтому так как
            $q$ положительно определённая, то $q_{|U}$ так же положительно определена.
            \\
            \begin{lemma}
                Если $q(x)  = x^T A x > 0$, тогда  $\det A > 0$.
            \end{lemma}
            \begin{proof}
                Приведём матрицу $A$ к каноническому виду. Тогда верно
                равенство $A = C^T E C$(причём $С$ невырожденная), $E$ тут получилось,
                так как $q$ положительно определена, значит $A = C^T C \Rightarrow \det A =
                \det C^T C = \underbrace{(\det C)^2}_{\not= 0} > 0$.
            \end{proof}
            Благодаря лемме из $q_{|U} > 0$ следует, что главная подматрица размера $k$
            имеет положительный определитель, следовательно $d_i > 0$.
    \end{itemize}
\end{proof}
\subsection{Евклидовы пространства}
\begin{definition}
    $V$~--- векторное пространство над $\mathbb{R}$, $\langle\cdot, \cdot\rangle\colon V\times V \mapsto R$~---
    скалярное произведение (билинейная положительно определённая форма). $\Rightarrow$ говорим, что на $V$ задано Евклидово пространство.
\end{definition}
\begin{motivation}
    Скалярное произведение позволяет нам строго задать понятие расстояния и даже угла в нашем пространстве.
\end{motivation}
\begin{remark}
    Заметим, что выбранное скалярное произведение позволяет нам говорить о расстоянии
    и угле между векторами в алгебраических понятиях.
    \[
        \begin{gathered}
        \norm{x} = \sqrt{\langle x, x \rangle}\\
        \rho(x, y) = \norm{x - y}
        \end{gathered}
    .\]
\end{remark}
\begin{remark}
    Из курса матанализа знаем, что в данной ситуации есть неравенство Коши-Буняковского.
    \[
        \abs{\langle x, y \rangle} \le \norm{x} \norm{y}
    \]
    Заметим, что из него следует следующее неравенство, которое мы в дальнейшем используем
    в определении угла.
    \[
        -1 \le \frac{\langle x, y \rangle}{\norm{x}\norm{y}} \le 1
    \]
\end{remark}
\begin{definition}
    Определим $\cos \varphi = \frac{\langle x, y \rangle}{\norm{x}\norm{y}}$. $\varphi$~--- угол между $x, y$, он определён на $[0, \pi]$.
\end{definition}
\begin{motivation}
    Косинус часто применяется для проецирования. Давайте об этом и поговорим.
\end{motivation}
\begin{definition}
    Определим проекцию на вектор $v\in V$, следующим образом: 
    $\pr_v\colon  V\mapsto \langle v \rangle$. 
\end{definition}
\begin{example}
    \begin{itemize}
        \item 
        \begin{figure}[H]
            \centering
            \incfig[0.5]{13.1}
            \label{fig:13.1}
        \end{figure}
        \[
            \pr_v(x) = \frac{\norm{x} \cdot \langle x, v \rangle }{\norm{x}\norm{v}}\cdot\frac{v}{\norm{v}}=
            \frac{ \langle x, v \rangle }{\norm{v}^2} \cdot v
        \]
        Отдельно стоит отметить, что мы получили координату в пространстве 
        $\langle v \rangle$, что в большинстве ситуаций удобно.
        \item 
        Давайте посмотрим как будет выглядеть проекция не на $v$, а на $v^{\perp}$,
        который лежит в одной плоскости с $v, x$(необходимо для того, чтобы он был
        задан однозначно). Хочется определить синус, но это не так просто сделать,
        поэтому посмотрим на картинку и заметим более простую формулу для
        требуемой проекции.
        \[
            \pr_{v^\perp}(x) = x - \pr_v(x) = x - \frac{\langle x, v \rangle}{\norm{v}^2} \cdot v
        \]
    \end{itemize}
\end{example}
\subsection{Ортогонализация Грама-Шмидта}
Есть некоторый набор векторов $e_1,\dots, e_k$~--- линейно независимый в Евклидовом пространстве $V$.
Хотим получить $f_1, \dots, f_k$~--- набор векторов такой что: 
\begin{enumerate}
    \item $f_i\perp f_j, i\not = j.$
    \item $\langle e_1,\dots, e_i \rangle = \langle f_1, \dots , f_i \rangle, \forall i$.
    \item  $\norm{f_i} = 1$
\end{enumerate}
\begin{definition}
    Ортогональный набор векторов это тот, который удовлетворяет 1.
\end{definition}
\begin{definition}
    Ортонормированный набор векторов это тот, который удовлетворяет 1,3.
\end{definition}
\begin{remark}
    Изложенный ниже алгоритм будет похож на диагонализацию матрицы квадратичной формы и это
    не случайно, он решает почти ту же задачу.
\end{remark}
\begin{theorem}[Ортогонализация Грама-Шмидта]
    Для линейно независимого набора векторов $e_1,\dots, e_k$
    существует ортонормированный набор $f_1, \dots, f_k$, для которого верно:\\
    $\langle e_1,\dots, e_i \rangle = \langle f_1, \dots , f_i \rangle, \forall i$.
\end{theorem}
\begin{proof}
    Процесс ортогонализации организуем по индукции.

    База: Должно выполняться $\langle e_1 \rangle = \langle f_1 \rangle, \norm{f_1} = 1$, тогда 
    $f_1 = \frac{e_1}{\norm{e_1}}$ подходит на эту роль.

    Переход: пусть мы уже построили набор векторов $f_1, \dots, f_{i - 1}$, хотим найти $f_i$.
    Тогда предположим, что существует $f_i$ в следующем виде: $f_i = c e_i + c_1 f_1 + \dots + c_{i - 1}f_{i - 1}$.
    Сделаем себе послабление и пока не будем хотеть, чтобы $\norm{f_i} = 1$,
    так как можно просто поделить потом всё на норму найденного вектора. Исходя
    из этого понимаем, что не умаляя общности $c = 1$.
    Тогда ищем $f_i$ в таком виде:
    \begin{equation}\label{eq:13:1}
        f_i = e_i + c_1 f_1 + \ldots + c_{i - 1} f_{i - 1}.
    \end{equation}

    Вспомним, что $f_i$ должно удовлетворять: $f_i \perp f_j$, тогда это необходимое условие
    переписывается в следующем виде благодаря линейности $\langle \cdot, \cdot \rangle$: 
    \[
        0 \stackrel{?}{=} \langle f_i, f_j \rangle = \langle e_i, f_j \rangle + c_j \langle f_j, f_j \rangle, \forall j < i
    \]
    Легко заметить что мы однозначно выразили коэффициент $c_j$
    \[
        c_j = - \frac{\langle e_i, f_j \rangle}{\norm{f_j}^2}
    \]
    Получаем, что свойство 1(ортогональность) мы заработали по построению.

    Поймём про свойство 2.
    По индукции имеем $\langle f_1, \ldots, f_{i - 1} \rangle  = \langle e_1, \ldots, e_{i - 1} \rangle$.
    Из равенства (\ref{eq:13:1}) $e_i$ выражается через $f_1, \dots, f_i$, 
    аналогично $f_i$ выражается через $f_1,\dots, f_{i-1}, e_i$, откуда следует
    требуемое равенство $\langle f_1,\dots,f_i \rangle = \langle e_1,\dots,e_i \rangle$.

    Для выполнения свойства 3(нормированность) просто поделим каждый вектор на его норму:
     \[
         f_i = \frac{f_i}{\norm{f_i}}
    .\] 
\end{proof}
\begin{remark}
    Верно даже для бесконечных пространств, где есть правильное понятие сходимости.
\end{remark}

Пусть у нас есть $x, U = \langle e_1, \dots, e_k \rangle$.
Хотим посчитать $\pr_U(x) = x - \ort (e_1\dots e_k, x)[k + 1]$, где $\ort(v_1,\dots,v_n)$ 
это результат ортогонализации, запущенной на векторах $v$.
(На самом деле в этой записи поделили лишний раз на $\norm{x}$ при ортогонализации, чего
не следовало делать, на лекции не придумали, как это удачно записать).

Благодаря приобретённым знаниям мы теперь умеем считать $\rho(x, U)$(расстояние от точки до подпространства)
и $\rho(x_0 + U, y_0 + W) = \rho(x_0 - y_0, U + W)$ (расстояние между двумя афинными подпространствами).
% TODO: пояснить равество расстояний
