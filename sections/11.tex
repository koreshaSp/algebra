\section{Лекция 16.04}
\subsection{Асимптотическая оценка координат произвольной матрицы}
\begin{motivation}
    На прошлой лекции получили очень мощное утверждение, про существование Жордановой формы
    для любой матрицы, причём единственной с точность до перестановки блоков.
\end{motivation}
$A \in M_n(\mathbb{C})$ Хотим понять как ведёт себя последовательность элементов
$A^k v_0 = v_k$, а именно как она себя ведёт при $k \to \infty$. Заметим, что
в ситуации, когда оператор $A$ диагонализуем, ответ мы уже 
\hyperref[stm:Ассимптотика диагонализуемого оператора]{знаем}. Пользуясь, тем что 
\hyperref[thm:О жордановом базисе]{всегда существует Жорданов базис}, получаем 
$\exists f\colon [A]^f_f = J \Leftrightarrow \exists C\colon A = C J C^{-1}$, где
$e$~--- стандартный базис, $f$~--- Жорданов базис, а
$C = [id]^f_e $ это матрица перехода из стандартного базиса в Жорданов. Тогда просто
по \hyperref[def:Матрица отображения]{определению} координат оператора, матрицу $C$ можно выразить следующим образом:
$C = \big( [f_1]_e, \dots, [f_n]_e \big)$. Про матрицу $C^{-1}$ тоже всё понятно, она
просто равна $[id]^e_f$ по одному из 
\hyperref[stm:О свойстве матриц линейных отображений]{свойств} матрицы линейного отображения.
Тогда $A^k v_0 = C J^k C^{-1} v_0$, значит можно просто понять как ведёт себя $J^k$ при
$k \to \infty$, поведение $C, C^{-1}$ нас не особо интересует, так как они не зависят от
$k$ и их можно воспринимать как константные множители.

Давайте научимся возводить $J$ в большую степень, благо это не сложно благодаря её 
специальному виду. В первую очередь заметим, что мы можем разбить матрицу $J$ на
Жордановы блоки, возвести в степень их, после чего слепить из них матрицу $J^k$.
Посмотрим как ведёт себя Жорданов блок при возведении в степень.
\[
    \begin{gathered}
        J_m(\lambda)^k = 
        \begin{pNiceMatrix}
             \lambda & 1 & 0 & \Cdots & 0\\
             0 & \Ddots & \Ddots & \Ddots & \Vdots\\
             \Vdotsfor{2}&\Ddots& & & 0\\
             & & & & 1\\
             0 & \Hdotsfor{2} & 0 & \lambda\\
         \end{pNiceMatrix}^k = 
         (\lambda E + N)^k 
         =\lambda^k E +  \lambda^{k - 1} \binom{k}{1} N + \dots + \lambda^{k - i} \binom{k}{i} N^i + \dots + N^k
    \end{gathered}
\] 
В последнем переходе мы воспользовались формулой бинома Ньютона, так как произведение матриц
$N, E$ является коммутативным. Теперь, глядя на эту сумму заметим, что нам интересны
только первые $m - 1$ слагаемых, так как
$N^m = 0$(\hyperref[fix:nilpotent]{уже доказывали}). Нетрудно показать, что $N^i$ это матрица
на $i$ побочной диагонали стоят единички, а во всех остальных элементах нули. Доказывается очевидно,
(см картинку).

\[
    \setlength{\extrarowheight}{-4mm}
    N = \begin{pNiceMatrix}[nullify-dots]
         0 & 1 & & & & \Block{2-1}<\LARGE>{0}\\
         & \Ddots & \Ddots\\
         \\\\
         \Block{2-2}<\LARGE>{0} & & & & & 1\\
                                & & & & & 0\\
     \end{pNiceMatrix}
     \quad
     N^2 = \begin{pNiceMatrix}[nullify-dots]
         0 & 0 & 1 & & & \Block{2-1}<\LARGE>{0}\\
         & \Ddots & \Ddots & \Ddots \\
         \\
         & & & & & 1\\
         \Block{2-2}<\LARGE>{0} & & & & & 0\\
                                & & & & & 0\\
     \end{pNiceMatrix}
     \quad
     N^3 = \begin{pNiceMatrix}[nullify-dots]
         0 & 0 & 0 & 1 & & \Block{2-1}<\LARGE>{0}\\
         & \Ddots & \Ddots & \Ddots & \Ddots \\
         & & & & & 1\\
         & & & & & 0\\
         \Block{2-2}<\LARGE>{0} & & & & & 0\\
                                & & & & & 0\\
     \end{pNiceMatrix} \quad N^4 = \dots
\] 
Тогда уже можно увидеть чему равна написанная ранее сумма:
    \NiceMatrixOptions{cell-space-limits = 5pt}
\[
    J_m(\lambda)^k = (\lambda E + N)^k =
    \begin{pNiceMatrix}[nullify-dots, parallelize-diags=false]
        \lambda^k & \binom{k}{1}\lambda^{k-1} & \binom{k}{2}\lambda^{k-2} & \Cdots & \binom{k}{m-1}\lambda^{k-m-1}\\
            & & & & \Vdots \\
            & & & & \binom{k}{2}\lambda^{k-2}\\
            & & & & \binom{k}{1}\lambda^{k-1}\\
            & & & & \lambda^k\\
    \CodeAfter 
        \line{1-1}{5-5}
        \line{1-2}{4-5}
        \line{1-3}{3-5}
    \end{pNiceMatrix}
\]
Откуда видно, что все коэффициенты этой клетки это $\mathcal{O}(k^{m-1} \lambda^k)$, 
при условии если оценить $\binom{k}{m - 1} = \mathcal{O}(k^{m-1})$. 
Эта оценку можно так же переписать следующим образом: 
$\mathcal{O}((\lambda + \epsilon)^{k}), \forall \epsilon$ сонаправленного с $\lambda$.
Эта оценка будет весьма полезна нам, когда мы будем искать максимальное по
модулю собственное число и собственный вектор матрицы.

\begin{statement}[Ассимптотическая оценка элементов матрицы]
    $A \in M_n(\mathbb{C})$ у матрицы $A$ есть собственное число $\lambda$, которое
    не кратное и максимальное по модулю. Тогда $v = c_1 f_1 + \dots + c_n f_n$, где
    $f$ это Жорданов базис, а $f_1$~--- собственный вектор для $\lambda$. Тогда
    $A^k v = c_1 \lambda^k f_1 + o(\lambda^k)$. Эту запись следует понимать как:
    коэффициенты при других базисных векторах есть $o(\lambda_k)$.
\end{statement}
\begin{proof}
    Для того, чтобы найти коэффициенты в Жордановом базисе сперва логично перейти в него. 
    Тогда вместо матрицы $A$ возникает её Жорданова форма $J$, а вместо вектора $v$ возникает
    столбец его координат в Жордановом базисе, тогда исходное равенство в новых координатах
    записывается следующим образом: 
    $J^k \begin{psmallmatrix} c_1\\\vdots\\c_n \end{psmallmatrix}$. Распишем $J^k$ и 
    воспользуемся тем, что $\lambda$ не кратное и максимальное по модулю. То, что оно
    не кратное означает, что в $J$ есть только одна Жорданова клетка с числом $\lambda$
    и её размер равен $1$ все элементы этой клетки после возведения в степень
    можно оценить как $\mathcal{O}(\lambda^k)$. То, что происходит в других Жордановых клетках нас не
    особо интересует, так как по рассуждению ранее в этом разделе можно понять, что это
    оценивается как $o(\lambda^k)$.
    Тогда наше произведение матриц можно записать в следующем виде:
    \[
     J^k \begin{pmatrix} c_1\\\vdots\\c_n \end{pmatrix} 
     =
     \begin{pNiceMatrix}
            \Block[borders={bottom,right}]{1-1}{\lambda^k} & & & \Block{2-1}<\LARGE>{0}\\
            & \Block[borders={bottom,right,top, left}]{1-1}{o(\lambda^k)} \\
            \Block{2-1}<\LARGE>{0} & & \Ddots\\
            & & & \Block[borders={top, left}]{1-1}{o(\lambda^k)} \\
     \end{pNiceMatrix}
     \begin{pmatrix} c_1\\\vdots\\c_n \end{pmatrix} = 
     \begin{pmatrix}
         \lambda^k c_1\\
         o(\lambda^k)\\
         \vdots\\
         o(\lambda^k)\\
     \end{pmatrix}
    \] 
    Таким образом, получили чего и хотели(при переходе в исходный базис).
\end{proof}
\subsection{Приближённое решение системы линейных уравнений}
Теперь подумаем в каких ранее возникших задачах помогает наше рассуждение.
Оказывается, что при помощи утверждения выше, можно очень быстро находить приближённое
решение СЛУ.

\begin{statement}[Итерационный метод решения системы линейных уравнений]
    Пусть мы хотим решить систему уравнений: $x = Ax + b \Leftrightarrow (E - A)x = b$.
    Если все собственные числа $A$ меньше единицы по модулю, то из этого следует, что последовательность
    $x_{i + 1} = Ax_i + b$, а $x_0 \neq 0$ было взято случайно, то последовательность $x_i$ 
    будет сходиться к единственному решению СЛУ.
\end{statement}
\begin{proof}
    Докажем, что у системы есть единственное решение. Заметим, что это следует из того,
    что $(E - A)$ является обратимой, так как в этом случае $x = (E - A)^{-1} b$.

    Докажем, что $E - A$ обратима.
    Все собственные числа $\lambda_i$ матрицы $A$ меньше единицы, следовательно,
    собственные числа $E - A$ имеют вид $1 - \lambda_i$(легко проверить самому)
    и не равны нулю, значит $\ker E - A = 0$
    (так как вектор в ядре это собственный вектор с собственным числом $0$), откуда 
    \hyperref[stm:Базовый критерий обратимости]{следует} обратимость $E - A$.

    На данный момент поняли, что у такой системы одно решение, давайте обозначим его за $x$.
    Теперь посмотрим на $h_i = x_i - x$, если покажем, что $h_i \to o(1)$, то из этого
    будет следовать решение исходной задачи. Выпишем рекурренту в терминах $h_i$:
    \[
        A h_i = A x_i - A x = \underbrace{x_{i + 1} - b}_{Ax_i} - \underbrace{x + b}_{Ax} = 
        x_{i + 1} - x = h_{i + 1}
    \] 
    Видим, что наша рекуррента устроена очень просто, поэтому её можно преобразовать
    к следующему виду: $h_i = A^i h_0$. По \hyperref[stm:Ассимптотическая оценка элементов матрицы]
    {прошлому утверждению} понимаем, что $A^i h_0 = \mathcal{O}(\lambda^{i + 1})$, где $\lambda$
    это максимальное собственное число $A$, но так как мы знаем, что $\lambda < 0$,
    то $A^i h_0 = o(1), i \to \infty \Rightarrow h_i \to 0$.
\end{proof}
\begin{remark}
    Заметим, что скорость сходимости достаточно приличная.
\end{remark}

\subsection{Применение итерационного метода для решения прикладных задач}
\textbf{Модель Леонтьева}

Является моделью экономического роста. Есть товары, количество каждого товара, которые
мы производим(допустим в год) обозначим за $x_i$. Они вместе образуют вектор $x$.
Кроме этого у нас есть какие-то технологии, за счёт которых мы можем производить одни
товары за счёт других. Таким образом, $a_{i,j}$~--- сколько товара номер $i$ необходимо, чтобы сделать
единицу $j$. Тогда $Ax$ это то, сколько каждого товаров нам необходимо иметь в начале года,
чтобы через год мы произвели $x$. Далее представим себе, что есть $y$ это набор товаров,
которые мы в конце года продаём. Тогда, для того, чтобы система была самоподдерживающейся
необходимо, чтобы было выполнено следующее уравнение:
\[
    x = Ax + y
\] 
То есть мы должны произвести $x$, такой, что будет достаточно для производства $x$
в следующем году плюс продать $y$. Заметим, что получаем в системе уравнений, которая
в точности подходит для \hyperref[stm:Итерационный метод решения системы линейных уравнений]
{итерационного метода, который мы прошли выше}.
Заметим, что в данной ситуации выполнено даже больше: $A, x, y$ имеют положительные координаты.
Откуда следует, что у этой системы всегда есть положительное решение, при условии, что 
собственные числа $\lambda_i$ матрицы $A$ меньше единицы.

\textbf{Pagerank}

Вспомним задачу про \hyperref[fix:pagerank]{pagerank} в ней необходимо найти 
решение системы $w = Pw$, где $P$ это оператор случайного блуждания. Тогда очевидно,
что мы ищем $w$ как собственный вектор с собственным числом равным $1$. Его можно найти
примерно следующей процедурой. Берётся произвольное $w_0$, после чего смотрим на
$P^kw_0$, при $k\to \infty$. Если собственное число $1$ максимальное по модулю
и не кратное, то по \hyperref[stm:Ассимптотическая оценка элементов матрицы]
{ассимптотической оценке матрицы}, $P^k w_0 = w + o(1)$. Остаётся только понять, когда
выполняется это условие на собственные числа матрицы $P$.

Несложно показать, что все собственные числа матрицы $P$ по модулю меньше или равны $1$.
Это следует из того, что сумма всех элементов в столбце $P$ равны единице. Так как 
подобная ситуация возникает достаточно часто, вынесем результат сразу в теорему.

\begin{theorem}[Гершгорина]
    $A\in M_n(\mathbb{C})$, определим $r_i = \sum\limits_{j\neq i}^{}{\abs{a_{i,j}}}$.
    Тогда все собственные числа $A$ лежат в объединении следующих кругов $B_{r_i}(a_{i,i})$.
\end{theorem}
\begin{proof}
    Давайте возьмём какое-нибудь собственное число $\lambda$ и сопутствующий ему собственный
    вектор $v$. Тогда возьмём $i\colon v_i$ максимальная по модулю координата $v$.
    Давайте покажем, что $\lambda \in B_{r_i}(a_{i,i})$ расписав равенство $Av = \lambda v$
    для $v_i$.
    \[
        \begin{gathered}
            \lambda v_i = \sum\limits_{j=0}^{n}{a_{i,j} v_j} \Leftrightarrow 
            (\lambda - a_{i,i})v_i = \sum\limits_{j\neq i}^{}{a_{i,j} v_j} \Rightarrow
            \abs{\lambda - a_{i,i}}\abs{v_i} = \abs{\sum\limits_{j\neq i}^{}{a_{i,j} v_j}}
            \le \sum\limits_{j\neq i}^{}{\abs{a_{i,j}} \abs{v_j}} \
            \underset{\abs{v_i} \ge \abs{v_j}}{\le}
            \sum\limits_{j\neq i}^{}{\abs{a_{i,j}} \abs{v_i}}\\
            \Downarrow\\
            \abs{\lambda - a_{i,i}} \le \sum\limits_{j\neq i}^{}\abs{a_{i,j}}
        \end{gathered}
    \] 
\end{proof}
\begin{definition}
    $B_{r_i}(a_{i,i})$ из предыдущей теоремы называются кругами Гершгорина.
\end{definition}
\begin{statement}
    Собственные числа $A$ совпадают с собственными числами $A^T$.
    Более того, $\chi_A(t) = \chi_{A^T}(t)$
\end{statement}
\begin{proof}
    Очевидно, \hyperref[thm:Свойства определителя]{так как} $\det A = \det A^T$.
\end{proof}

Заметим, что если $P$ матрица \hyperref[def:Марков]{Марковского процесса}, тогда сумма элементов в каждом столбце
равна $1$. Значит $r_i + a_{i,i} = 1$.

Тогда, возвращаясь к задаче про pagerank, можно заметить, что $A = P^T$, то есть в $A$
сумма в каждой строке ровно $1$. А значит можно применить \hyperref[thm:Гершгорина]{теорему Гершгорина} для $A^T$,
получив, что $\lambda < a_{i,i} + r_i$. А уже благодаря тому,
что $a_{i,i} + r_i = 1$, то получаем $\lambda < 1$.

\begin{statement}[Перрона]
    $A \in M_n(\mathbb{R})\colon a_{i,j} > 0$, тогда существует единственное максимальное
    по модулю собственное число $\lambda$ не кратное. Более того, для $\lambda$ есть
    собственный вектор, все координаты которого положительны.
\end{statement}
\begin{proof}
     Сказали смотреть в другом конспекте.
\end{proof}
