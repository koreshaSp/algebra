\section{Лекция 11.04}
\begin{motivation}
    В прошлый раз мы пользовались линейным оператором (линейное отображение из пространства в себя), чтобы определить понятие ориентации. Теперь посмотрим, в каких ситуациях естественно, что линейное отображение принимает объект какого-то фиксированного типа и выдает объект этого же фиксированного типа. $V\xrightarrow{L}V$
\end{motivation}
\begin{examples}
    \begin{enumerate}
        \item Преобразование плоскости/пространства, повороты.
        \item $K[x]_{\le n} \mapsto K[x]_{\le n}$, где  $f\rightarrow f'$
        \item Марковские цепи. \\
        \begin{definition}
            Цепь Маркова — последовательность случайных событий с конечным или счётным числом исходов, где вероятность наступления каждого события зависит только от состояния, достигнутого в предыдущем событии.
        \end{definition}
        \item Эволюция во времени.\\
            Пусть есть популяция, которая разделена по возрастам. Пусть $n_1, n_2,\dots, n_k$~--- количество особей каждого
            возраста. Пусть у нас заданы коэффициенты выживаемости, которые показывают, что особь возраста $n_i$ проживёт
            сколько надо и станет особью возраста $n_{i+1}$. Пусть $f_i$ количество потомков, которое даёт в среднем особь возраста $n_i$.
            $s_i$ коэффициент выживаемости. 
            Хотим понять, как будет выглядеть матрица перехода количества особей с учётом времени (для фиксированных $f_i, s_i$). Утверждается, что так($k = 4$):
            \[
            \begin{pmatrix}
                f_1&f_2&f_3&f_4\\
                s_1&0&0&0\\
                0&s_2&0&0\\
                0&0&s_3&0\\
            \end{pmatrix}
            \begin{pmatrix}
                n_1\\
                n_2\\
                n_3\\
                n_4\\
            \end{pmatrix}
             =
             \begin{pmatrix}
                 n'_1\\
                 n'_2\\
                 n'_3\\
                 n'_4
             \end{pmatrix}
            .\] 
            Эта конструкция называется моделью Лесли.
            \begin{remark}
                Модель Лесли - это не Марковская цепь, хотя и очень похожа.
            \end{remark}
        \item
            Различные операторы в физике. Например, оператор Лапласа $\delta f$
        \item
            Линейные рекуррентные соотношения.\\
            $x_{n + k} = a_{k-1}x_{n + (k - 1)} + \dots + a_0x_n$.
            Тогда:
            $X_{n + 1} = AX_n$, где матрицу $A$ можно составить из коэффициентов $a_{k - 1}, \dots, a_0$, $X_n = \begin{pmatrix}
                x_n \\ \vdots \\ \vdots \\ x_{n + k - 1}
            \end{pmatrix}$
    \end{enumerate}
\end{examples}
\subsection{Собственные числа и вектора}
\begin{motivation}
    Давайте посмотрим на одномерный случай. Тогда линейный оператор - это просто домножение на какую-то константу. В самом деле всё элементарно $Av = \lambda\cdot v$, где $\lambda\in K$.
    Тогда легко видеть, что $A^nv = \lambda^n v$. Хотим получить что-то похожее, но в пространствах больших размерностей.
\end{motivation}
\begin{definition}
    $L\colon V\rightarrow V, \lambda\in K, v\not= 0\in V$. Тогда $\lambda$~--- собственное число $L$, а $v$~--- собственный вектор $L$,
    если $Lv = \lambda v$.
\end{definition}
\begin{motivation}
    Почему это понятие удобно. Пусть $v$ какое-то распределение на вершинах графа, на котором есть случайное блуждание.
    Хотим вычислить $P^n v$. $v = c_1v_1 + \dots + c_kv_k$, где все $v_i$~--- собственные вектора $P$, которым соответствуют собственные числа $\lambda_i$. Утверждается, что $P^n v = \sum c_i \lambda_i^n v_i$
    Получили существование очень удобной формы.
\end{motivation}
\begin{remark}
    Заметим, что такое возможно не всегда.
    Пусть 
     \[
    A = 
    \begin{pmatrix}
        0& 1\\
        -1&0\\
    \end{pmatrix}
    \text{~--- матрица поворота на 270}\]
    Мы хотим, чтобы после применения преобразования к собственному вектору, этот вектор сохранил направление. Но понятно, что для матрцы $A$ такого не может быьть.
\end{remark}
\begin{motivation}
    Мы поняли, что не всегда можно построить базис из собственных векторов. \\
    Давайте научимся искать собственные числа и вектора.
\end{motivation}
\begin{definition}
    $L\colon V\mapsto V$. $\det L = \det[L]^e_e$, где $e$~--- произвольный базис.
\end{definition}
\begin{statement}
    $\lambda$~--- собственное число оператора $L \Leftrightarrow \det(L - \lambda E) = 0$
\end{statement}
\begin{proof}
    $\exists v\not= 0\colon Lv = \lambda v$, что эквивалентно $\exists v\not=0\colon (L - \lambda E)v = 0$, это значит, что $v \in \ker L$, то есть $\dim \ker L \neq 0$, а это то же самое, что $\det(L - \lambda E) = 0$
\end{proof}
\subsection{Характеристический многочлен}
\begin{definition}
    $L\colon V\rightarrow V$. Тогда характеристическим многочленом
    оператора $L$ называется $\chi_L(t) = \det(L - tE)$.
    \[
        \begin{pmatrix}
            a_{1,1}-t&a_{1,2}&\dots&a_{1,n}\\
            a_{2,1} & \ddots & \ddots& \vdots\\
            \vdots & \ddots & \ddots& a_{n-1,n}\\
            a_{n,1} & \dots & a_{n,n-1}& a_{n,n}-t\\
        \end{pmatrix}
    .\] 
\end{definition}
\begin{remark}
    Старший коэффициент это $(-1)^n$. Это легко видеть, так как максимальная степень $t$, которая может получиться в выражении про определитель, берется из произведения элементов на главной диагонали.
\end{remark}
\begin{remark}
    $[L - tE]^e_e \in M_n(K(t))$. Так удобно говорить, чтобы не зависеть от введённого базиса $e$ (было доказано ранее, что $\det [L - t E]^e_e$ не зависит от выбора базиса).
    Из этого следует, что $\chi_L(t)$ действительно многочлен от $t$ над полем $K(t)$.
\end{remark}
\begin{definition}
    $A\in M_n(K)$. Сумма диагональных элементов называется следом матрицы $A$ и обозначается $Tr A$.
\end{definition}
\begin{remark}
    Свободный член $\chi_L(t)$ это $\det L$, а при $t^{n-1}$ это $(-1)^{n-1}Tr L$.
\end{remark}
\begin{proof} Сначала покажем, что свободный член $\chi_L(t) = \det L$. \\ $\chi_L(t) = \det (L - t E)$, а свободный член $\chi_L(t) = \chi_L(0) = \det (L - 0) = \det L$. \\
Теперь покажем, что коэффициент $\chi_L(t)$ при $t^{n - 1}$ равен $(-1)^{n - 1}Tr L$. \\
Вспомним формулу для определителя: $\det A = \sum\limits_\sigma \prod\limits_i a_{i\sigma(i)}$ и общую идею, что мы смотрим на все возможные расстановки ладей на матрице, чтобы они не били друг друга. Так как мы смотрим на коэффициент при $t^{n - 1}$, хотя бы $n - 1$ ладья должна стоять на диагонали, ну а так как матрица квадратная, мы не можем поставить последнюю ладью куда-то кроме последней свободной клетки на главной диагонали. Теперь понятно, что мы смотрим на выражение вида $(a_{1 1} - t)\cdot(a_{2 2} - t)\cdot \dots \cdot (a_{n n} - t)$, из которого, раскрывая скобки, получим необходимое равенство.
\end{proof}
\begin{remark}
    Свойства следа:
    \begin{enumerate}
        \item $Tr (A + B) = Tr A + Tr B$
        \item След матрицы не зависит от выбора базиса
        \item $Tr (AB) = Tr (BA)$, где $A \in M_{n \times m}(K), \; B \in M_{m \times n}(K)$
    \end{enumerate}
\end{remark}
\begin{statement}
    (частный случай свойства следа 3). Если $C$~--- обратимая, а $A$~--- квадратная, тогда:
    $Tr(C A C^{-1}) = Tr A$
\end{statement}
\begin{proof}
    $A$~--- линейный оператор, $C = [id]^e_f$~--- матрица перехода из стандартного базиса $e$ в какой-то базис $f$.
    Тогда для произвольного $u \colon C A C^{-1} = [id]^e_f[A]_e^e[id]_e^f[u]_f = [Au]_f$. С другой стороны: $[A]_e^e[u]_e = [Au]_e$
    А так как след матрицы не зависит от выбора базиса, то равенство верно.
\end{proof}
\begin{example}
    \[
    A = 
    \begin{pmatrix}
        a&b\\
        c&d
    \end{pmatrix}
    .\] 
    Тогда $\chi_A(t) = t^2 - Tr A\cdot t + \det A$
\end{example}
\subsection*{Диагонализуемость матриц}
\begin{motivation}
    Давайте подумаем существует ли ситуация, где мы можем рассчитывать на то, что любой вектор раскладывается в сумму собственных.
    Для этого нужно иметь базис из собственных векторов.
\end{motivation}
\begin{theorem}
    $L: V\rightarrow V, v_1\dots, v_k, \lambda_1\dots, \lambda_k$~--- собственные вектора и собственные числа $L$ соответственно, причём
    $\lambda_i\not= \lambda_j \forall i\not= j$.
    Тогда:  $v_1\dots v_k$ линейно независимые
\end{theorem}
\begin{proof}
    Докажем от противного. Предположим что они линейно зависимы. Тогда возьмём наименьшую возможную по количеству элементов
    нетривиальную линейную комбинацию(опускаем максимальное число слагаемых, равных нулю).
    $c_1v_1+\dots+c_k v_k = 0$. Давайте применим к правой и левой части $L$. Получим, следующее равенство
    $0 = c_1\lambda_1v_1+\dots + c_k\lambda_k v_k$. Не умаляя общности $c_1\not=0$ (так как есть ненулевое слагаемое).
    Теперь давайте сделаем следующий трюк: умножим исходное равенство на $\lambda_1$ и вычтем из текущего, тогда получим:
    \[
    0 = 0\cdot v_1 + (\lambda_1 - \lambda_2)c_2v_2 + \dots + (\lambda_1 - \lambda_k) c_k v_k
    .\]
    В этой сумме есть тоже ненулевое слагаемое, так как $\lambda_i \not= \lambda_j$ но это значит, что мы получили
    разложение нуля с меньшим количеством ненулевых слагаемых. Противоречие.
\end{proof}
\begin{follow}
    $L\colon V \mapsto V$, где $V$ векторное пространство над алгебраически замкнутым полем $K$(значит, что количество корней
    любого многочлена с учетом кратности равно его степени). Причём все корни $\chi_L(t)$~--- различны.
    Тогда $\exists \text{базис из собственных векторов}$.
\end{follow}
\begin{proof}
    Пусть $\lambda_1\dots \lambda_n$~--- корни многочлена $\chi_L(t)$(их количество равно размерности пространства $n$).
    $v_1\dots v_n$~--- собственные вектора, соответствующие $\lambda_1\dots \lambda_n$, а значит линейно независимы по предыдущему следствию. Значит, $v_1\dots v_n$ --- базис 
\end{proof}
\begin{remark}
    Условие того, что у линейного оператора есть базис из собственных векторов называется диагонализуемость линейного оператора.
\end{remark}
\begin{definition}
    $L\colon V \to V$~--- диагонализуема, если $\exists$ базис $e\colon [L]^e_e$~--- диагональная матрица.
    \[
        L = 
        \begin{pmatrix}
            \lambda_1&&0\\
            &\ddots &\\
            0& & \lambda_n
        \end{pmatrix}
    .\] 
    Если переписать на язык матриц, то получим: \\ $A\in M_n(K)$ --- диагонализуема $\Leftrightarrow \exists C\in GL_n(K) \colon C^{-1}AC = 
    \begin{pmatrix}
        \lambda_1&&0\\
        &\ddots &\\
        0& & \lambda_n
    \end{pmatrix} $
\end{definition}
\newpage
\begin{definition}
    $L\colon V\mapsto V$, $\lambda$~--- собственное число. Тогда:
    \begin{enumerate}
        \item Алгебраическая кратность $\lambda$ --- 
            кратность  $\lambda$ как корня $\chi_L(t)$.
        \item Геометрическая кратность $\lambda$ ---
            $\dim \ker (L-\lambda E)$
    \end{enumerate}
\end{definition}
\begin{theorem}[Необходимое и достаточное условие диагонализуемости]
    $L\colon V\mapsto V$ 
    \begin{enumerate}
        \item $\chi_L(t)$~--- раскладывается на $n$ линейных множителей в $K$.
        \item алгебраическая кратность $\lambda$ = геометрическая кратность $\lambda$, $\forall\lambda\text{~--- собственное число}$ 
    \end{enumerate}
    Тогда и только тогда $L$~--- диагонализуемая.
\end{theorem}
\begin{proof}
    $L$~--- диагонализуема $\ora$ свойства.
    \begin{enumerate}
        \item 
            Понятно, что $\chi_L(t)$ и $\dim \ker L$ можно считать выбрав любой базис.
            Выберем $e$~--- базис из собственных векторов $L$. 
            \[
              [L]^e_e = \begin{pmatrix}
                \lambda_1 & 0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
                0 & \ddots & & & & & & & & \vdots\\
                \vdots & & \lambda_1 & & & & & & & \vdots\\
                \vdots & & & \lambda_2 & & & & & & \vdots\\
                \vdots & & & & \ddots & & & & & \vdots\\
                \vdots & & & & & \lambda_2 & & & & \vdots\\
                \vdots & & & & & & \ddots & & & \vdots\\
                \vdots & & & & & & & \lambda_n & & \vdots\\
                \vdots & & & & & & & & \ddots & 0\\
                0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 & \lambda_n \\
              \end{pmatrix}
            \]
            Мы выбираем такой порядок векторов в базисе $e$, чтобы базисные векторы, которые соответствуют одному и тому же собственному числу находились рядом. \\
            У такой матрицы удобно считать характеристический многочлен, его корнями будут просто элементы главной диагонали. Алгебраическая кратность $\lambda_i \; \forall i$ - это количество раз, которое $\lambda_i$ встречается на диагонали. Обозначим алгебраическую кратность $\lambda_i$ через $m_i$. \\
            Покажем, что геометрическая кратность $\lambda_i$ тоже равна $m_i$. Для $i = 1$ посмотрим на 
            \[
              \dim \ker ([L]^e_e - \lambda_i E) = \begin{pmatrix}
                0 & 0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
                0 & \ddots & & & & & & & & \vdots\\
                \vdots & & 0 & & & & & & & \vdots\\
                \vdots & & & \lambda_2 - \lambda_1 & & & & & & \vdots\\
                \vdots & & & & \ddots & & & & & \vdots\\
                \vdots & & & & & \lambda_2 - \lambda_1 & & & & \vdots\\
                \vdots & & & & & & \ddots & & & \vdots\\
                \vdots & & & & & & & \lambda_n - \lambda_1 & & \vdots\\
                \vdots & & & & & & & & \ddots & 0\\
                0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 & \lambda_n - \lambda_1 \\
              \end{pmatrix}
            \]
            Легко видеть, что $\lambda_i - \lambda_1 \neq 0 \; \forall i \neq 1$, а ядро - это подпространство, натянутое на первые $m_1$ координат (мы просто обнулили первые $m_1$ диагональных элементов). Аналогично можно проделать для любого $i$.
    \end{enumerate}
    Теперь докажем $\ola$.
    
    Возьмём $\ker(L - \lambda_i E)$. Пусть $\dim \ker (L - \lambda_i E) = k_i$, $u_{1,i},\dots,u_{k_i,i}$~--- базис $\ker(L - \lambda_i E)$. 
    Благодаря условиям теоремы(знаем, что количество корней
    $\chi_L(t) = n$, а значит сумма алгебраических кратностей равна $n$, ну а сумма 
    геометрических кратностей равна им и тоже равна $n$) мы знаем, что $\sum k_i = n = \dim L$.
    Тогда: 
    \[
        \begin{gathered}
            \sum\limits_{i,j}c_{i,j}u_{i,j} = 0 \Leftrightarrow
            \sum\limits_{j}\sum\limits_{i}c_{i,j}u_{i,j} = 0\\
            \text{заметим, что } \sum\limits_{i}c_{i,j}u_{i,j} = 0 \text{ это сумма собственных векторов с собственным числом } \lambda_j \\
            \text{сумма собственных векторов это либо } 0,\\ \text{либо собственный вектор с  тем же самым собственным числом}
        \end{gathered}
    .\] 
    Если какой-то элемент суммы --- ноль, отбросим его. Тогда у нас останется  сумма собственных векторов для различных собственных чисел, она линейно независима. А если она равна нулю, значит все эти элементы должны были быть равны нулю с самого начала.
\end{proof}
\begin{remark}
    $f(x_1,\dots, x_n)\not= 0\in \R[x_1\dots x_n]$, то вероятность попасть в множество нулей многочлена равна нулю
    $P[f(x_1,\dots, x_n) = 0]$.
\end{remark}
\begin{remark}
    $f(x) = a_0 + \dots + a_nx^n$ имеет два одинаковых комплексных корня равносильна тому, что 
     $\exists D(a_0,\dots, a_n) = 0$. $D$ называется дискриминантом. \\
     Дополнительный факт(упражнение, подсказка: определитель Вандермонда):
     \[
         D = a_n^{2n - 2}\prod\limits_{}^{}{(\lambda_i - \lambda_j)^2}
     \]
\end{remark}
\begin{motivation}
    Давайте поймем, что происходит в ситуации $A^n v$.
\end{motivation}
\begin{statement}
    Пусть $A\in M_n(\C)$~--- диагонализуемая, $v\in \C^n$, тогда $A^kv = c_1\lambda_1^kv_1 +
    \mathcal{O}(\lambda_2^k)$($k\rightarrow \infty$), где $\abs{\lambda_1}\ge \abs{\lambda_2} \ge \dots \ge \abs{\lambda_n}$.
\end{statement}
\begin{proof}
    Очевидно, так как 
    \[
        A^k v = c_1\lambda_1^k v_1 + \dots + c_n \lambda_n^k v_n
    .\] а благодаря тому, что $\abs{\lambda_1} \ge \abs{\lambda_i}, \forall i \not= 1$ все члены, кроме первого могут быть
    записаны как $\mathcal{O}(\lambda_2^k)$.
\end{proof}
\begin{follow}
    \[
        \begin{gathered}
            \lambda_1 \sim \frac{A^{k + 1}v[1]}{A^k v[1]}\\
            \frac{A^kv}{\norm {A^k v}}\rightarrow cv_1
        \end{gathered}
    .\] 
    Таким образом мы можем считать приближённо значение собственных чисел.
\end{follow}
