\section{Лекция 18.04}
\begin{motivation}
    Закроем наш старый долг из теоремы \hyperref[thm:О аннулирующем операторе]{о аннулирующем операторе}.
    Хотим выяснить какой многочлен подходит на роль $g(A)$ в системе:
    $g(A) = 0, (p,q) = 1, g = pq$. Тогда пользуясь нашей теоремой мы сможем представить
    $V$ как прямую сумму: $\ker p(A) \oplus \ker q(A)$.
\end{motivation}
\begin{theorem}[Гамильтона-Кели]
   $\chi_A(A) = 0$
   Где $K$~--- алгебраически замкнутое(не существенное, есть несложное сведение).
\end{theorem}
\begin{proof}
    Пусть $e$~--- Жорданов базис, тогда $J = [A]^e_e$.
    Хотим показать, что $\chi_A(J) = 0$.     Выпишем вид этого многочлена: 
    $\chi_A(t) = \chi_J(t) = \pm\prod\limits_{\lambda_i}^{}{(t-\lambda_i)^{\alpha_i}}$,
    где предполагается, что все $\lambda_i$ различны.
    Так как матрица $J$ блочно-диагональная
    то можем посчитать её характеристический многочлен от каждого блока, а далее просто
    сложить их вместе. На языке матриц это выглядит следующим образом:
    \[
        \chi_J(J) = 
            \begin{pNiceMatrix}
            \Block[borders={bottom,right}]{1-1}{\chi_J(J_{k_1}(\lambda_1))} & & & \Block{2-1}<\LARGE>{0}\\
                                                                            & \Block[borders={bottom,right,top, left}]{1-1}{\chi_J(J_{k_2}(\lambda_2))} \\
            \Block{2-1}<\LARGE>{0} & & \Ddots\\
            & & & \Block[borders={top, left}]{1-1}{\chi_J(J_{k_n}(\lambda_n))} \\
            \end{pNiceMatrix}
    \] 
    
    Заметим, что $\alpha_i$ больше или равна максимальному размеру Жордановой клетки
    для $\lambda_i$.
    Тогда можем посчитать следующее:
    $\chi_J(J_{k_i}(\lambda_i))$, где $k_i \le \alpha_i$.
    Давайте посмотрим на множитель $(J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i}$,
    из произведения $\chi_J(J_{k_i}(\lambda_i))$. 

    \[
        (J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i} =
        \begin{pNiceMatrix}[nullify-dots]
         0 & 1 & & \Block{2-1}<\LARGE>{0}\\
         & \Ddots & \Ddots\\
         \Block{2-2}<\LARGE>{0} & & & 1\\
                                & & & 0\\
        \end{pNiceMatrix}^{\alpha_i}
        [\alpha_i \ge k_i] = 0
    \] 

    Расписав его, легко
    видеть, что это верхнетреугольная матрица с нулями на диагонали,
    возведённая в степень $\alpha_i \ge k_i$. Следовательно, этот множитель
    равен нулю, что доказывалось \hyperref[fix:nilpotent]{ранее}.
    Получили: 
    \[
        (J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i} = 0 \Rightarrow
        \chi_J(J_{k_i}(\lambda_i)) = 0 \Rightarrow
        \chi_J(J) = 0
    .\]
\end{proof}

\subsection{Билинейные формы и понятие ортогональности}
\begin{definition}
    $V$~--- векторное пространство.
    Пусть $h\colon V \times V \mapsto K$ причём полилинейное, тогда $h$ будем называть
    билинейной формой.
\end{definition}
\begin{remark}
    $e$~--- базис.
    Тогда $h(e_i, e_j)$ однозначно характеризуют нашу билилейную форму и наоборот.
    Из этого возникает следующее определение.
\end{remark}
\begin{definition}
    Матрица $A$ заданная как: $A_{i,j} = h(e_i, e_j)$ называется матрицой билинейной формы в базисе $e$.
    Более того, верна следующая формула:
    \[
        h(u, v) = [u]^T_eA[v]_e
    .\] 
\end{definition}
\begin{proof}
Докажем правильность формулы выше, положив $x = [u]_e, y = [v]_e$
 \[
    h(u, v) = x^T A y
.\] 
В этой ситуации верно: $u = \sum x_ie_i, v = \sum y_i e_i$ по определению.
Расписав произведение матриц, получим, что:
\[
    h(u, v) = \sum\limits_{i, j}^{}{x_i y_j h(e_i, e_j)}
.\] 
\end{proof}
\begin{motivation}
    Чаще всего билинейные формы возникают вместе со скалярным произведением 
    в $\R^n$. Причём скалярное произведение тут стоит понимать в общем плане для
    нормированных пространств, например $C\left([0, 1]\right) f, g \rightarrow \int\limits_{0}^{1}{f(x)g(x)\ dx}$
    тоже является скалярным произведением. Более того, можно это довести до следующего
    общего вида:
    $\int\limits_{0}^{1}{f(x)g(x) w(x)\ dx}$, где $w\colon [0, 1]\mapsto \R_{> 0}$,
    причём непрерывная.
\end{motivation}
\begin{definition}
    \item Достаточно часто(но не всегда) возникает симметричность
        $h(u,v) = h(v,u), \forall u,v$ 
\end{definition}
\begin{remark}
    Оказывается не так уж и просто придумать пример несимметричного
    (но вот такое например годиться):
    \[
        x^T
        \begin{pmatrix}
            0 & -1\\
            1 & 0\\
        \end{pmatrix} y
    \]
\end{remark}
\begin{remark}
    $h$~--- симметричное тогда и только, когда $A$~--- симметричная матрица.
\end{remark}
\begin{proof}
    Если для $A$ верно: $A = A^T$. Тогда посмотрим на выражения $x^T A y$ и $y^T A x$.
    Так как оба этих выражения дают число, то мы можем транспонировать и получить то же 
    самое число, таким образом: 
    \[
        x^T A y = (x^T A y)^T = y^T A^T (x^T)^T = y^T A x
    .\]

    В другую сторону очевидно просто по определению матрицы $A$, так как $a_{i,j} = h(e_i, e_j)$.
\end{proof}
\begin{definition}
    Форма называется положительно определённой, если $h(x, x) > 0, \forall x\not=0$
\end{definition}
\begin{remark}
    Удобство положительно определённой формы позволяет легко раскладывать пространства в прямую сумму.
    Например можно думать о разложении  $\R^2$ на два одномерных пространства. Пусть у нас есть
    одномерное подпространство $l_0$. Хотим разложить наше пространство на прямую сумму $l_0$
    и ещё какого-нибудь. Технически вариантов для второго подпространства бесконечно много(показаны
    штрихами на рисунке), но в ситуации, когда у нас есть положительно определённая билинейная
    форма у нас возникает один канонический вариант $l_1$, ортогональный исходному $l_0$. Как раз
    про определение ортогональности в произвольном пространстве и то, почему 
    форма должна быть положительно определённой, мы сейчас и поговорим.
    \begin{figure}[H]
        \centering
        \incfig{12.1}
        \label{fig:12.1}
    \end{figure}
\end{remark}
\begin{definition}
    $x \perp_h y$(вектор $x$ ортогонален вектору $y$) если $h(x,y) = 0$, где $h$~--- симметричная 
    билинейная форма.
\end{definition}
\begin{definition}
    Пусть $h$~--- симметричная билинейная форма, $U\le V$, то $U^{\perp} = 
    \{x\in V \mid  x\perp_h y, \forall y \in U \}$ называется ортогональным дополнением.
\end{definition}
\begin{theorem}
    $K = \R$, $h$~--- симметричное и положительно определённое, $U \le V$,
    тогда $V = U \oplus U^{\perp}$.
\end{theorem}
\begin{proof}\leavevmode
    По определению прямой суммы проверим два пункта.
    \begin{enumerate}
        \item
        $U\cap U^{\perp} = \{0\}$
        Так как если предположить противное $x\in U \cap U^\perp$, тогда 
        $x \in U, x \in U^\perp \Rightarrow x \perp_{h} x \Rightarrow 
        h(x, x) = 0$. Получили противоречие с положительной определённостью $h$.
        \item 
        $U + U^\perp = V$. Благодаря первому пункту можно доказывать только
        $\dim U + \dim U^{\perp} = \dim (U + U^\perp) = \dim V$,
        из чего будет следовать требуемое равенство, по 
        \hyperref[thm:О размерности подпространств]{теореме о размерности подпространств}.

        Докажем два неравенства. Заметим, что в одну сторону неравенство легко получить
        из первого пункта по \hyperref[thm:Формула Грассмана]{формуле Грассмана} 
        \[
             \dim U + \dim U^{\perp} = \dim (\underbrace{U + U^\perp}_{\le V}) + \underbrace{\dim (U \cap U^\perp)}_{=0} \le \dim V
        \]

        Для доказательства в другую сторону надо показать, что $U^{\perp}$ является достаточно
        большим. По определению $U^\perp$ имеем $h(x, y) = 0, \forall y\in U \Rightarrow x\in U^\perp$, а это значит, что наше
        пространство задаётся при помощи $k \le \dim U$ уравнений по замечанию ниже.

        \begin{remark}
            Если $e_1,\dots, e_k$~--- базис $U$, 
            $x\in U^{\perp}$ эквивалентно тому, что $x \perp e_i, \forall i$.
        \end{remark}
        Доказывается данное замечание очевидным образом, расписывая $h$ по определению.

        То, каким числом уравнений описывается $U^\perp$ даёт нам следующее неравенство:
        $\dim U^{\perp} = \dim V - k \ge \dim V - \dim U$, что и требовалось.
    \end{enumerate}
\end{proof}
\subsection{Квадратичные формы и их диагонализируемость}
\begin{remark}
    Заметим, что в ситуации с подстановкой одинаковых элементов получаем следующее:
    $h(x,x) = x^TAx$, что является однородным многочленом степени 2.
\end{remark}
\begin{definition}
    $V$~--- векторное пространство над $K$. $q\colon V\mapsto K$. 
    в некоторой системе координат это однородный многочлен степени 2.
    Тогда $q$~--- квадратичная форма.
\end{definition}
Благодаря замечанию выше, очевидно что по любой билинейной форме можно построить
квадратичную.
\begin{remark}
    Предполагается, что $char K \not= 2$ сейчас и далее.
\end{remark}
Давайте поймём, что любой билинейной форме однозначно соответствует квадратичная.
\begin{remark}
    Квадратичный многочлен в координатах имеет следующий вид:
    $q(x) = \sum\limits_{i\le j}^{}{b_{i,j} x_i x_j}$
    Более того, матрица $B$ является симметричной.
\end{remark}
Хотим связать квадратичные формы с билинейными.
 \[
 a_{i,j} = 
 \begin{cases}
     \frac{b_{i,j}}{2}, i < j\\
     \frac{b_{j,i}}{2}, i > j\\
     b_{i,i}, i = j
 \end{cases}
.\] 

В чём же удобство такого определения? Примерно в следующем:
\begin{enumerate}
    \item
        $x^TAx = q(x)$
    \item
         $A = A^T$
\end{enumerate}
Получаем, что существует взаимно-однозначное сопоставление между
симметричными билинейными формами и квадратичными формами.

\begin{statement}
     \[
			 h(x, y) = \frac{q(x + y) - q(x) - q(y)}{2}
    .\] 
\end{statement}
\begin{proof}
    упражнение ;)
\end{proof}
\begin{motivation}
    Хотим навести какую нибудь классификацию для случая, когда $K = \R$.
    Для этого посмотрим как меняются квадратичные и билинейные формы при переходе из одной матрицы в другую.
\end{motivation}
Поймём как матрица квадратичной формы меняется при переходе в другую систему координат.
$q$~--- квадратичная форма. $e$~--- базис в котором $q$ задаётся матрицей $A$,
$f$~--- базис в котором $q$ задаётся матрицей $B$.
Давайте выразим $x$ следующим образом: $x = Cy$, где $C = [id]^f_e$.
Тогда можно записать $q$ в виде: $q(x) = x^T A x = (Cy)^T AC y = y^T (C^TAC) y$.
Следовательно, при переходе из $e$ в другой базис $f$ матрица $A$ переходит в $([id]^f_e)^T A [id]^f_e$.

Давайте теперь попробуем сформулировать факт, гарантирующий существование простого вида
произвольной матрицы $A$ хотя бы в каком-то базисе. Утверждается, что всегда можно
достигнуть того, чтобы матрица $A$ стала диагональной. А именно

\[
     A = 
     \begin{pmatrix}
         \lambda_ 1 & & 0 \\
        & \ddots & \\
        0& & \lambda_n
     \end{pmatrix}
.\] 
Тогда $q(x) = \sum\limits_{i=1}^{n}{\lambda_i x_i^2}$.
В такой ситуации можно утверждать о попарной ортогональности собственных векторов,
так как вне диагональные элементы это как раз значения $h(x,y)$ на базисных векторах.

\begin{definition}
    $e$~--- базис $V$. $h\colon V\times V\rightarrow K$, причём $h$~--- симметричная.
    Тогда будем называть базис  $e$ ортогональным, если $e_i\perp_h e_j, \forall i\not=j$.
\end{definition}
\begin{theorem}
    $q\colon V \mapsto K$~--- квадратичная форма. Тогда существует базис $e$ такой, что
    матрица квадратной формы диагональна.
\end{theorem}
\begin{proof}
    Распишем форму в координатах:
    \[
        q(x) = \sum\limits_{}^{}{a_{i,j} x_i x_j} = a_{1,1}x_1^2 + 2a_{1,2}x_1x_2 + \dots + 2a_{1,n}x_1x_n +
        g(x_2,\dots, x_n)
    \]
    Хотим, запихнуть всё то, что содержит $x_i$ под знак квадрата.
    Тогда:
    \[
		q(x) = a_{1,1}\left(x_1 + \frac{a_{1,2}}{a_{1,1}}x_2 + \dots + \frac{a_{1,n}}{a_{1,1}}x_n\right)^2 -
        \hat{g}(x_2,\dots, x_n) + g(x_2,\dots, x_n)
    .\]
    
    Убедимся, что матрица перехода в новой системе координат не вырожденная
     \[
         \begin{gathered}
             y_1 = x_1 + \frac{a_{1,2}}{a_{1,1}}x_2 + \dots + \frac{a_{1,n}}{a_{1,1}}x_n\\
             y_2 = x_2\\
             \vdots\\
             y_n = x_n
         \end{gathered}
    .\] 
    Тогда получаем следующую матрицу перехода координат:
    \[
    y = 
    \begin{pNiceMatrix}
        1 & \frac{a_{1,2}}{a_{1,1}} & \Hdotsfor{2} & \frac{a_{1,n}}{a_{1,1}}\\
        0 & \Ddots &   0    & \Cdots & 0\\
   \Vdots & \Ddots & \Ddots & \Ddots & \Vdots\\
   \Vdots &        & \Ddots & \Ddots & 0\\
        0 & \Cdots & \Cdots &   0    & 1\\
    \end{pNiceMatrix}
    x
    .\] 

    Но всё вышеперечисленное работает, когда $a_{1,1}\not= 0$.
    Давайте доразберём частные случаи:
    \begin{enumerate}
        \item
        Если $\exists a_{i,i} \neq 0$, тогда можно взять $y_1 = x_i, y_i = x_1$, где $x_i \neq 0$.
        \item
            Если $a_{i,i} = 0,\forall i$. Тогда допустим если $a_{1,2}\not=0$, тогда сделаем замену:
        	$x_1 = y_1 + y_2, x_2 = y_1 - y_2, y_i = x_i, i \geq 3$

			Получим $2a_{1,2}$ при $y_1^2$ и $-2a_{1,2}$ при $y_2^2$ и продолжим наш алгоритм.
		\item Если все $a_{1,i} = 0$, то форма не зависит от $x_1$ и можно смело пропустить его
    \end{enumerate}
\end{proof}

Заметим, что преобразования которым мы подвергаем матрицу во время выполнения 
алгоритма из доказательства очень похожи на элементарные преобразования
алгоритма Гаусса. За одну итерацию мы делаем около $n$ элементарных
преобразований для строчек и для столбцов.
\subsection{Свойства миноров и сигнатура квадратичной формы}
\begin{theorem}
    $q(x) = x^T Ax$. Пусть $d_i$~--- $i$ главный минор($d_0=1$). Тогда если $d_i \not= 0,\forall i$, на диагонали
    будут стоять следующие скаляры $\frac{d_i}{d_{i - 1}}$.
\end{theorem}
\begin{proof}
    Будет доказано на следующей лекции пользуясь теоремой с сегодняшней лекции.
\end{proof}
