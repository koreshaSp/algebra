\section{Лекция 18.04}
\begin{motivation}
    Закроем наш старый долг из теоремы \hyperref[thm:О аннулирующем операторе]{о аннулирующем операторе}.
    Хотим выяснить какой многочлен подходит на роль $g(A)$ в системе:
    $g(A) = 0, (p,q) = 1, g = pq$. Тогда пользуясь нашей теоремой мы сможем представить
    $V$ как прямую сумму: $\ker p(A) \oplus \ker q(A)$.
\end{motivation}
\begin{theorem}[Гамильтона-Кели]
   $\chi_A(A) = 0$
   Где $K$~--- алгебраически замкнутое(не существенное, есть несложное сведение).
\end{theorem}
\begin{proof}
    Пусть $e$~--- Жорданов базис, тогда $J = [A]^e_e$.
    Хотим показать, что $\chi_A(J) = 0$.     Выпишем вид этого многочлена: 
    $\chi_A(t) = \chi_J(t) = \pm\prod\limits_{\lambda_i}^{}{(t-\lambda_i)^{\alpha_i}}$,
    где предполагается, что все $\lambda_i$ различны.
    Так как матрица $J$ блочно-диагональная
    то можем посчитать её характеристический многочлен от каждого блока, а далее просто
    сложить их вместе. На языке матриц это выглядит следующим образом:
    \[
        \chi_J(J) = 
            \begin{pNiceMatrix}
            \Block[borders={bottom,right}]{1-1}{\chi_J(J_{k_1}(\lambda_1))} & & & \Block{2-1}<\LARGE>{0}\\
                                                                            & \Block[borders={bottom,right,top, left}]{1-1}{\chi_J(J_{k_2}(\lambda_2))} \\
            \Block{2-1}<\LARGE>{0} & & \Ddots\\
            & & & \Block[borders={top, left}]{1-1}{\chi_J(J_{k_n}(\lambda_n))} \\
            \end{pNiceMatrix}
    \] 
    
    Заметим, что $\alpha_i$ больше или равна максимальному размеру Жордановой клетки
    для $\lambda_i$.
    Тогда можем посчитать следующее:
    $\chi_J(J_{k_i}(\lambda_i))$, где $k_i \le \alpha_i$.
    Давайте посмотрим на множитель $(J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i}$,
    из произведения $\chi_J(J_{k_i}(\lambda_i))$. 

    \[
        (J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i} =
        \begin{pNiceMatrix}[nullify-dots]
         0 & 1 & & \Block{2-1}<\LARGE>{0}\\
         & \Ddots & \Ddots\\
         \Block{2-2}<\LARGE>{0} & & & 1\\
                                & & & 0\\
        \end{pNiceMatrix}^{\alpha_i}
        [\alpha_i \ge k_i] = 0
    \] 

    Расписав его, легко
    видеть, что это верхнетреугольная матрица с нулями на диагонали,
    возведённая в степень $\alpha_i \ge k_i$. Следовательно, этот множитель
    равен нулю, что доказывалось \hyperref[fix:nilpotent]{ранее}.
    Получили: 
    \[
        (J_{k_i}(\lambda_i) - \lambda_i)^{\alpha_i} = 0 \Rightarrow
        \chi_J(J_{k_i}(\lambda_i)) = 0 \Rightarrow
        \chi_J(J) = 0
    .\]
\end{proof}

\subsection{Билинейные и квадратичные формы}
\begin{definition}
    $V$~--- пространство.
    Пусть $h\colon V \times V \mapsto K$ причём полилинейное, тогда $h$ будем называть
    билинейной формой.
\end{definition}
\begin{remark}
    $e$~--- базис.
    Тогда $h(e_i, e_j)$ однозначно характеризуют нашу билилейную форму и наоборот.
    Из этого возникает следующее определение.
\end{remark}
\begin{definition}
    Матрица заданная как: $a_{i,j} = h(e_i, e_j)$ называется матрицой билинейной формы в базисе $e$.
    Более того, верна следующая формула:
    \[
        h(u, v) = [u]^T_eA[v]_e
    .\] 
\end{definition}
\begin{proof}
Докажем правильность формулы выше, положив $x = [u]_e, y = [v]_e$
 \[
    h(u, v) = x^T A y
.\] 
В этой ситуации верно: $u = \sum x_ie_i, v = \sum y_i e_i$ по определению.
Расписав произведение матриц, получим, что:
\[
    h(u, v) = \sum\limits_{i, j}^{}{x_i y_j h(e_i, e_j)}
.\] 
\end{proof}
\begin{motivation}
    Чаще всего билинейные формы возникают вместе со скалярным произведением 
    в $\R^n$. Причём скалярное произведение тут стоит понимать в общем плане для
    нормированных пространств, например $C\left([0, 1]\right) f, g \rightarrow \int\limits_{0}^{1}{f(x)g(x)\ dx}$
    тоже является скалярным произведением. Более того, можно это довести до следующего
    общего вида:
    $\int\limits_{0}^{1}{f(x)g(x) w(x)\ dx}$, где $w\colon [0, 1]\mapsto \R_{> 0}$,
    причём непрерывная.
\end{motivation}
\begin{definition}
    \item Достаточно часто(но не всегда) возникает симметричность
        $h(u,v) = h(v,u), \forall u,v$ 
\end{definition}
\begin{remark}
    Оказывается не так уж и просто придумать пример несимметричного
    (но вот такое например годиться):
    \[
        x^T
        \begin{pmatrix}
            0 & -1\\
            1 & 0\\
        \end{pmatrix} y
    \]
\end{remark}
\begin{remark}
    $h$~--- симметричное тогда и только, когда $A$~--- симметричная матрица.
\end{remark}
\begin{proof}
    Если для $A$ верно: $A = A^T$. Тогда посмотрим на выражения $x^T A y$ и $y^T A x$.
    Так как оба этих выражения дают число, то мы можем транспонировать и получить то же 
    самое число, таким образом: 
    \[
        x^T A y = (x^T A y)^T = y^T A^T (x^T)^T = y^T A x
    .\]

    В другую сторону очевидно просто по определению матрицы $A$, так как $a_{i,j} = h(e_i, e_j)$.
\end{proof}
\begin{definition}
    Форма называется положительно определённой, если $h(x, x) > 0, \forall x\not=0$
\end{definition}
\begin{remark}
    Удобство положительно определённой формы позволяет легко раскладывать пространства в прямую сумму.
    Например можно думать о разложении  $\R^2$ на два одномерных пространства. Пусть у нас есть
    вектор $x$.
    \begin{figure}[H]
        \centering
        \incfig{12.1}
        \label{fig:12.1}
    \end{figure}
    TODO(пример с двумя прямыми на плоскости(рисунок????))
\end{remark}
\begin{definition}
    $x \perp_h y$(вектор $x$ ортогонален вектору $y$) если $h(x,y) = 0$, где $h$~--- симметричная 
    билинейная форма.
\end{definition}
\begin{definition}
    Пусть $h$~--- симметричная билинейная форма, $U\le V$, то $U^{\perp} = 
    \{x\in V \mid \forall y \in U, x\perp y = 0\}$ называется ортогональным дополнением.
\end{definition}
\begin{theorem}
    $K = \R$, $h$~--- симметричное и положительно определённое, тогда 
    $U \le V \Rightarrow V = U \oplus U^{\perp}$.
\end{theorem}
\begin{proof}\leavevmode
    \begin{enumerate}
        \item
        $U\cap U^{\perp} = \{0\}$
        Так как если предположить противное, тогда $0 = h(x,x) > 0$, противоречие.
        \item 
        $\dim U + \dim U^{\perp} = \dim V$
        Докажем два неравенства. Заметим, что в одну сторону неравенство есть
        по первому пункту(а именно $\dim U + \dim U^{\perp} \le V$).

        Для доказательства другого неравенства надо показать, что $U^{\perp}$ является достаточно
        большим. Но просто по определению $h(x, y) = 0 \Rightarrow x\in U$, а это значит, что наше
        пространство задаётся при помощи $\dim U$ уравнений по замечанию ниже.

        \begin{remark}
            Если $e_1,\dots, e_k$~--- базис $U$, 
            $x\in U^{\perp}$ эквивалентно тому, что $x \perp e_i, \forall i$.
        \end{remark}

        А это значит, что так как $\dim U^{\perp} = \dim \ker \ge \dim (V - K)$, то требуемое
        неравенство $\dim U + \dim U^{\perp} \ge V$
    \end{enumerate}
\end{proof}
\begin{remark}
    Заметим, что в ситуации с подстановкой одинаковых элементов получаем следующее:
    $h(x,x) = x^TAx$, что является однородным многочленом степени 2.
\end{remark}
\begin{definition}
    $V$~--- векторное пространство над $K$. $q\colon V\mapsto K$. 
    в $\forall$ системе координат это однородный многочлен степени 2.
    Тогда $q$~--- квадратичная форма.
\end{definition}
\begin{remark}
    Предполагается, что $char K \not= 2$ сейчас и далее.
\end{remark}
\begin{remark}
    Квадратичный многочлен в координатах имеет следующий вид:
    $q(x) = \sum\limits_{i\le j}^{}{b_{i,j} x_i x_j}$
    Более того, матрица $B$ является симметричной.
\end{remark}
\begin{remark}
    Хотим связать квадратичные формы с билинейными.
     \[
     a_{i,j} = 
     \begin{cases}
         \frac{b_{i,j}}{2}, i < j\\
         \frac{b_{j,i}}{2}, i > j\\
         b_{i,i}
     \end{cases}
    .\] 
    В чём же удобство такого определения? Примерно в следующем:
    \begin{enumerate}
        \item
            $x^TAx = q(x)$
        \item
             $A = A^T$
    \end{enumerate}
    Получаем, что существует взаимо однозначное сопоставление между
    симметричными билинейными формами и квадратичными формами.
\end{remark}
\begin{statement}
     \[
    h(x, y) = \frac{1}{2}(g(x + y) - g(x) - g(y))
    .\] 
\end{statement}
\begin{proof}
    упражнение ;)
\end{proof}
\begin{motivation}
    Хотим навести какую нибудь классификацию для случая, когда $K = \R$.
    Для этого посмотрим как меняются квадратичные и билинейные формы при переходе из одной матрицы в другую.
\end{motivation}
\begin{statement}
    $q$~--- квадратичная форма. $e$~--- базис $x^TAx$, $f$~--- базис $x^TBy$.
    Давайте выразим $x$ следующим образом: $x = Cy$, где $C = [id]^f_e$.
    Откуда следует, что $(Cy)^TACy = y^T(C^TAC)y$
\end{statement}
\begin{statement}
    Пусть 
     \[
         A = 
         \begin{pmatrix}
             \lambda_ 1 & & 0 \\
            & \ddots & \\
            0& & \lambda_n
         \end{pmatrix}
    .\] 
    Тогда $q(x) = \sum\limits_{i=1}^{n}{\lambda_i x_i^2}$.

    После этого можем утверждать о попарной ортогональности собственных векторов.
    Так как вне диагональные элементы это как раз значения $h(x,y)$ на базисных векторах.
\end{statement}
\begin{definition}
    $e$~--- базис $V$. $h\colon V\times V\rightarrow K$, причём $h$~--- симметричная.
    Тогда будем называть базис  $e$ ортогональным, если $e_i\perp e_j, \forall i\not=j$.
\end{definition}
\begin{theorem}
    $q\colon V \mapsto K$~--- квадратичная форма. Тогда существует базис $e$ такой, что
    матрица квадратной формы диагональна.
\end{theorem}
\begin{proof}
    Распишем форму в координатах:
    \[
        q(x) = \sum\limits_{}^{}{a_{i,j} x_i x_j} = a_{1,1}x_1^2 + 2a_{1,2}x_1x_2 + \dots + 2a_{1,n}x_1x_n +
        g(x_2,\dots, x_n)
    \]
    Хотим, запихнуть всё то, что содержит $x_i$ под знак квадрата.
    Тогда выражение выше равно следующему:
    \[
        a_{1,1}\left(x_1 + \frac{a_{1,2}}{a_{1,1}}x_2 + \dots + \frac{a_{1,n}}{a_{1,1}}x_n\right)^2 -
        \hat{g}(x_2,\dots, x_n) + g(x_2,\dots, x_n)
    .\]
    
    Убедимся, что матрица перехода в новой системе координат не вырожденная
     \[
         \begin{gathered}
             y_1 = x_1 + \frac{a_{1,2}}{a_{1,1}}x_2 + \dots + \frac{a_{1,n}}{a_{1,1}}x_n\\
             y_2 = x_2\\
             \vdots\\
             y_n = x_n
         \end{gathered}
    .\] 
    Тогда получаем следующую матрицу перехода координат:
    \[
    y = 
    \begin{pmatrix}
        1 & \frac{a_{1,2}}{a_{1,1}} & \dots & \frac{a_{1,n}}{a_{1,1}}\\
          & 1 & &\\
          &  & \ddots &\vdots\\
          0 & & & 1\\
    \end{pmatrix} x
    .\] 

    Но всё вышеперечисленное работает, когда $a_{1,1}\not= 0$.
    Давайте доразберём частные случаи:
    \begin{enumerate}
        \item
        Если $\exists a_{i,i} = 0$, тогда можно взять $y_1 = x_i, y_i = x_1$, где $x_i \neq 0$.
        \item
            Если $a_{i,i} = 0,\forall i$. Тогда допустим если $a_{1,2}\not=0$, то можно сделать замену:
        $2a_{1,2} x_1 x_2 \leftrightarrow
        \begin{pmatrix}
            0 & a_{1,2}\\
            a_{1, 2} & 0
        \end{pmatrix}$
        Тогда ответ можно будет восстановить следующим образом:
        $x_1 = y_1 + y_2, x_2 = y_1 - y_2$.
    \end{enumerate}
\end{proof}
Давайте поймём, каким преобразованиям мы подвергаем матрицу во время выполнения алгоритма из доказательсва
выше.
TODO(подобнее) работает очень похоже на алгоритм Гаусса. За одну итерацию мы делаем около $n$ элементарных
преобразований для строчек и для столбцов.
\subsection{Свойства миноров и сигнатура квадратичной формы}
\begin{theorem}
    $q(x) = x^T Ax$. Пусть $d_i$~--- $i$ главный минор($d_0=1$). Тогда если $d_i \not= 0,\forall i$, на диагонали
    будут стоять следующие скаляры $\frac{d_i}{d_{i - 1}}$.
\end{theorem}
\begin{proof}
    Будет доказано на следующей лекции пользуясь теоремой с сегодняшней лекции.
\end{proof}
