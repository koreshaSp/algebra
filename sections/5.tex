\section{Лекция 14.03}
\subsection{Зависимость матрицы линейного отображения от выбранных базисов}
\begin{theorem}
    Пусть $L: U\mapsto V$~--- линейное отображение. 
    $e, e', f, f'$~--- базисы в $U, V$ соответственно.
    Знаем $[L]^e_f$(матрицу перехода при первом наборе базисов), хотим
    получить $[L]^{e'}_{f'}$.

    Утверждается, что тогда верна следующая формула:
    $[L]^{e'}_{f'}[u]_{e'}=[id]_{f'}^{f}[L]_f^{e}[id]_e^{e'}[u]_{e'}\forall u\in U$.
\end{theorem}
\begin{proof}
    $[id]_{e}^{e'}$~--- матрица перехода $e\rightarrow e'$.
    $[L]_f^e$~--- матрица $L$ в старых $x$
    $[id]_{f'}^f$~--- матрица перехода $f\mapsto f'$.
    Математически утверждение очевидно, надо лишь формально доказать.
\end{proof}
\begin{follow}
    Используя утверждение с прошлой пары и нашу новую теорему.
    $$\forall A\in M_{m\times n}(K).
    \exists C\in M_m(K), D\in M_n(K)\colon A = D^{-1}
    \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0\\
    \end{array}\right) C$$
    На самом деле $D, C$~--- матрицы перехода, более того они
    обратимые.
\end{follow}
\begin{proof}
    $x\rightarrow Ax$.
    $K^n\mapsto K^m$.
    $e, f$~--- старые(текущие) базисы.
    $e', f'$~--- новые базисы.
    $$A = 
    \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0
    \end{array}\right)
    $$
    $A = [id]\begin{pmatrix}
        E_r & 0\\
        0 & 0
    \end{pmatrix}[id]$
\end{proof}
\begin{remark}
    Такое разложение не единственное
    $$
    E_n = A^{-1}E_nA
    $$
\end{remark}
\begin{definition}
    Есть две матрицы, каждая из которых разделена на 4 блока.
    $$
    A = \left(\begin{array}{c|c}
            A_{1,1} & A_{1,2}\\
            \hline
            A_{2,1} & A_{2,2}
    \end{array}\right)
    B = \left(\begin{array}{c|c}
            B_{1,1} & B_{1,2}\\
            \hline
            B_{2,1} & B_{2,2}
    \end{array}\right)$$
    Тогда их можно перемножать поблочно.
    Берём столбец $A$ и перемножаем на строчку $B$.
    (дописать на условия размер $A_{1,1}$ соответствует размеру $B_{1,1}$)
    Тогда блочное умножение матриц будет выглядит следующим образом:
    $$
    \left(\begin{array}{c|c}
            A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2,2}\\
            \hline
            A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{1,2} + A_{2,2}B_{2,2}\\
    \end{array}\right)
    $$
\end{definition}
\begin{follow}
    $A\in M_{m\times n}$, тогда $\exists C\in M_{r\times n},
    D\in M_{m\times r}: A = DC$. Причём у $C$ линейно независимые строки,
    а у $D$~--- линейно независимые столбцы, что можно записать как $rk C = r, rkD = r$.
    (тут какая-то шляпа см запись)

    Ответ почему у $D$ и у $C$ линейно независимые строки/столбцы
    $$
    \begin{pmatrix}
        \dots
    \end{pmatrix}
    \times 0 = 0 \Rightarrow x = 0
    $$
    Так мы получили линейную комбинацию столбцов.

    Как получить линейную независимость строчек?
    $rkA = n\Rightarrow$ размер образа равен $n$.
    Мы говорим про линейную зависимость строк обратимой матрицы.
    Поговорим об этом в общей форме. 
    $A$~--- матрица, в которой линейно независимых строк $k$ штук.
    Сколько есть линейно независимых столбцов. Ответ: тоже $k$.
    Сейчас как раз займёмся этим вопросом.
\end{follow}
\subsection{Ранги транспонированных матриц}
\begin{definition}
    Транспонированием матрицы $A\in M_{m\times n}$ назовём
    матрицу $A^T\in M_{n\times m}$ со следующим свойством:
    $(A^T)_{i,j} = A_{j,i}$.
\end{definition}
\begin{statement}
    $rkA = rkA^T$.
\end{statement}
\begin{proof}\leavevmode
    \begin{enumerate}
        \item $A, B (A + B)^T = A^T + B^T$
        \item $\lambda, A (\lambda A)^T=\lambda A^T$
        \item $(AB)^T = B^TA^T$.
    \end{enumerate}
    Все эти свойства очевидны из определений. Будем пользоваться
    ими для доказательства.
    $A = D^{-1}\left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0
    \end{array}\right) C$. Тогда $A_T$ имеет вид:
    $A^T = C^T \left(\begin{array}{c|c}
            E_r & 0\\
            \hline
            0 & 0
    \end{array}\right)(D^{-1})^T$
    Для этого было бы неплохо знать, что \begin{enumerate}
        \item Докажем $(C^T)^{-1} = (C^{-1})^T$ если $C$~--- обратимая.
            $CC^{-1} = E_n$. Тогда: 
            $(C^{-1})^TC^T=E_n^T = E_n$.
            На самом деле это значит что: если $C$~--- обратимая, то
            $C^T$~--- обратимая.
        \item Хотим понять что происходит, когда мы считаем ранг
            произведения исходный матрицы на обратимую.
            Нам будет достаточно следующего неравенства:
            $rk AB \le min(rkA, rkB)$.
            Для этого докажем два факта $rkAB \le rkA$ и $rkAB\le rkB$.
            Первое очевидно, так как $Im A \supseteq ImAB$.
            Для доказательства второго надо посмотреть, что происходит с размерностью
            подпространств после линейного отображения. 
            $ImAB = A(ImB)$ Знаем, что $\abs{Im B} = rk B$.
            Давайте поймём, что после умножения на $A$ ранг
            не увеличится. Это верно, так как в базис $B$
            при умножении на $A$ не может перестать быть образующей.
            $\dim \ker L + \dim Im L = rkb$.
    \end{enumerate}
    $rkAC\le rkA$ это мы уже знаем
    С другой стороны $A = ACC^{-1}$, тогда 
    $rkA = rkACC^{-1}\le rk AC$. 
    Имея два строгих неравенства в разные стороны мы получили
    равенство $rk AC = rk A$.
\end{proof}
\begin{motivation}
    Переписать всё выше в порядке:
    $rkAB\le min(rkA, rkB)\Rightarrow$ если $C$~--- обратим
    $\Rightarrow rkAC = rkA$.
    Далее мы поняли, что $C$~--- обратимая $\Rightarrow 
    (C^T)^{-1}=(C^{-1})^T$, откуда используя ещё 
    $(AB)^T=B^TA^T$(надо доказать самим) следует
    $rkA = rk A^T$.
\end{motivation}
\begin{example}
    Попробуем использовать наши новые знания на практике.
    Надо назначить поисковой системе набор весов так, чтобы
    мы знали важность каждого сайта в нашей сети.
    $$W_i = \sum_{j\rightarrow i} \frac{w_j}{d_j^{out}}$$
    Хотим составить систему для всех $i$ и расписать
    её через матрицу. Будем думать, что столбец соответствует
    одному сайту из которого что-то исходит. В $j$ столбце
    стоят элементы следующего вида: $0, \frac{1}{d^{out}_j}$
    (в зависимости от того соединены элементы вершиной или нет)
    это все делается для того, чтобы мы могли описать систему
    в виде: $w = P\times w$, где $w$~--- вектор, а $P$~--- матрица
    (то есть мы просто записали на языке матриц уравнение выше).

    Но есть некоторая проблема: для выполнения инварианта
    (сумма исходящих рёбер для каждой вершины равно 1) необходимо
    добавить петли с недостающем весом.

    Давайте научимся выражать и считать событие вида: 
    пользователь был на сайте $v$ и $k$ раз перешёл на случайный
    сайт с текущего и нам необходимо посчитать вероятность
    оказаться в вершине $u$. Это имеет специальное название:
    <<модель случайного блуждания на графе>>. 

    Вопрос о системе $w = P\times w$. А почему вообще у этой
    системы есть нетривиальное распределение? У нас есть
    замечательное тождество на матрицу $P$: сумма в каждом
    столбце равна 1. Запишем это в следующем виде:
    $$\big(1,1,\dots, 1\big)\times P = \big(1,1,\dots, 1\big)$$
    Ну а если транспонировать обе части, то получим:
    $$
    P^T\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 
    \begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix}
    $$
    Если перенести всё в левую часть:
    $$
    (P^T - E_n^T)\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 0
    $$
    Используя свойство транспонирования:
    $$
    (P - E_n)^T\begin{pmatrix}
        1\\\vdots\\1
    \end{pmatrix} = 0
    $$
    Отсюда $rk(P - E_N)^T<n \Rightarrow rk(P - E_n)<n$.
    Значит $\dim \ker(P - E_n) \ge 1$, ну а если $w\not=0$,
    а значит $Pw -  E_nw = 0\Rightarrow Pw = w$.
    Что и требовалось доказать.

    На самом деле ответ у нас очень часто ещё и единственный
    (то есть $\ker(P-E_n)=1$). Пример, когда не единственный $P = E_n$.
\end{example}
\subsection{Связь умножения матриц с элементарными преобразованиями в алгоритме Гаусса}
\begin{motivation}
    Хотим в равенстве вида:
    $A = D^{-1}\big(\dots\big)C$
    задать специальные классы матриц, такие, что если
    $C,D$ принадлежат им, то система имеет единственное решение.
\end{motivation}
\begin{definition}
    Вернетреугольной матрицей назовём матрицу вида:$$\begin{pmatrix}
        * & * & \cdots & *\\
        \vdots & * & \ddots & \vdots\\
        \vdots & \ddots & \ddots & *\\
        0 & \cdots& \cdots& *
    \end{pmatrix}
    $$
    Нижнетреугольной назовём матрицу вида: $$\begin{pmatrix}
        * & 0 & \cdots & 0\\
        \vdots & * & \ddots & \vdots\\
        \vdots & \ddots & \ddots & 0\\
        * & \cdots& \cdots& *
    \end{pmatrix}
    $$

    Обозначим эти классы как: $UT_n(k)$ и $LT_n(K)$ соответственно.
\end{definition}
\begin{remark}
    Заметим, что при работе метода Гаусса
    можно заменить элементарное преобразование первого типа на 
    домножение на <<почти единичную>> матрицу. (todo написать вид матрицы)
\end{remark}
\begin{proof}
    Заметим, что при умножении на матрицу до $i$ строчки
    (где стоит $\lambda$) ничего не меняется в результате произведения.(todo написать строже)
    Строчка $i$ даст нам то, что надо, а все строчки большие $i$ опять таки
    ничего не меняют
\end{proof}
\begin{definition}
    Будем называть эту <<почти единичную>> матрицу матрицей
    преобразования первого типа.
\end{definition}
\begin{motivation}
    Теперь хотим двигаться в сторону преобразования второго и
    третьего типа.
\end{motivation}
TODO: был спойлер, что матрица 3 типа выглядит как:
$$
\begin{pmatrix}
    1 & 0 & 0 & 0 & \dots & 0\\
    0 & 1 & 0 & 0 & \dots & 0\\
    0 & \dots & 0 & \lambda & \ddots & 1\\
    0 & \dots & 0 & 0 & \vdots & 1
\end{pmatrix}
$$
